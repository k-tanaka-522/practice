AWSTemplateFormatVersion: '2010-09-09'
Description: 'File Upload System with S3, Lambda image processing, and CDN'

Parameters:
  EnvironmentName:
    Type: String
    Default: dev
    AllowedValues: [dev, staging, prod]
    Description: Environment name

  ProjectName:
    Type: String
    Default: file-upload
    Description: Project name for resource naming

  MaxFileSize:
    Type: Number
    Default: 10485760  # 10MB
    MinValue: 1048576   # 1MB
    MaxValue: 104857600 # 100MB
    Description: Maximum file size in bytes

  AllowedFileTypes:
    Type: CommaDelimitedList
    Default: 'jpg,jpeg,png,gif,pdf,doc,docx'
    Description: Allowed file extensions

  EnableImageProcessing:
    Type: String
    Default: 'true'
    AllowedValues: ['true', 'false']
    Description: Enable automatic image processing

  ThumbnailSizes:
    Type: CommaDelimitedList
    Default: '150x150,300x300,800x600'
    Description: Thumbnail sizes to generate (widthxheight)

  EnableVirusScan:
    Type: String
    Default: 'false'
    AllowedValues: ['true', 'false']
    Description: Enable virus scanning (requires additional setup)

Conditions:
  IsProduction: !Equals [!Ref EnvironmentName, 'prod']
  EnableImageProc: !Equals [!Ref EnableImageProcessing, 'true']
  EnableVirus: !Equals [!Ref EnableVirusScan, 'true']

Resources:
  # KMS Key for S3 encryption
  FileUploadKMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: !Sub 'KMS key for ${ProjectName} ${EnvironmentName} file encryption'
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: Allow S3 access
            Effect: Allow
            Principal:
              Service: s3.amazonaws.com
            Action:
              - kms:Decrypt
              - kms:GenerateDataKey
            Resource: '*'
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  FileUploadKMSKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub 'alias/${ProjectName}-${EnvironmentName}-file-upload'
      TargetKeyId: !Ref FileUploadKMSKey

  # S3 Bucket for original uploads
  UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-${EnvironmentName}-uploads-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: aws:kms
              KMSMasterKeyID: !Ref FileUploadKMSKey
            BucketKeyEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteIncompleteUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 1
          - Id: TransitionToIA
            Status: Enabled
            Transition:
              StorageClass: STANDARD_IA
              TransitionInDays: 30
          - Id: TransitionToGlacier
            Status: !If [IsProduction, Enabled, Disabled]
            Transition:
              StorageClass: GLACIER
              TransitionInDays: 90
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders: ['*']
            AllowedMethods: [GET, PUT, POST, DELETE, HEAD]
            AllowedOrigins: ['*']  # Restrict in production
            MaxAge: 3000
            ExposedHeaders: [ETag]
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt FileProcessorFunction.Arn
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # S3 Bucket for processed files
  ProcessedBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-${EnvironmentName}-processed-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: aws:kms
              KMSMasterKeyID: !Ref FileUploadKMSKey
            BucketKeyEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: false
        BlockPublicPolicy: false
        IgnorePublicAcls: false
        RestrictPublicBuckets: false
      VersioningConfiguration:
        Status: Enabled
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders: ['*']
            AllowedMethods: [GET, HEAD]
            AllowedOrigins: ['*']
            MaxAge: 3000
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # S3 Bucket Policy for processed files
  ProcessedBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ProcessedBucket
      PolicyDocument:
        Statement:
          - Sid: PublicReadGetObject
            Effect: Allow
            Principal: '*'
            Action: s3:GetObject
            Resource: !Sub '${ProcessedBucket}/*'
            Condition:
              StringEquals:
                's3:ExistingObjectTag/public': 'true'

  # DynamoDB Table for file metadata
  FileMetadataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub '${ProjectName}-${EnvironmentName}-file-metadata'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: fileId
          AttributeType: S
        - AttributeName: userId
          AttributeType: S
        - AttributeName: uploadedAt
          AttributeType: S
        - AttributeName: fileType
          AttributeType: S
        - AttributeName: status
          AttributeType: S
      KeySchema:
        - AttributeName: fileId
          KeyType: HASH
      GlobalSecondaryIndexes:
        - IndexName: UserFilesIndex
          KeySchema:
            - AttributeName: userId
              KeyType: HASH
            - AttributeName: uploadedAt
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
        - IndexName: FileTypeIndex
          KeySchema:
            - AttributeName: fileType
              KeyType: HASH
            - AttributeName: uploadedAt
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
        - IndexName: StatusIndex
          KeySchema:
            - AttributeName: status
              KeyType: HASH
            - AttributeName: uploadedAt
              KeyType: RANGE
          Projection:
            ProjectionType: ALL
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES
      PointInTimeRecoverySpecification:
        PointInTimeRecoveryEnabled: true
      SSESpecification:
        SSEEnabled: true
        KMSMasterKeyId: !Ref FileUploadKMSKey
      TimeToLiveSpecification:
        AttributeName: expiresAt
        Enabled: true
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # IAM Role for Lambda functions
  FileProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${EnvironmentName}-file-processor-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: FileProcessorPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectTagging
                  - s3:PutObject
                  - s3:PutObjectTagging
                  - s3:DeleteObject
                Resource:
                  - !Sub '${UploadBucket}/*'
                  - !Sub '${ProcessedBucket}/*'
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource:
                  - !GetAtt UploadBucket.Arn
                  - !GetAtt ProcessedBucket.Arn
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:DeleteItem
                  - dynamodb:Query
                  - dynamodb:Scan
                Resource:
                  - !GetAtt FileMetadataTable.Arn
                  - !Sub '${FileMetadataTable.Arn}/index/*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:GenerateDataKey
                Resource: !GetAtt FileUploadKMSKey.Arn
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref FileProcessingTopic
              - Effect: Allow
                Action:
                  - rekognition:DetectLabels
                  - rekognition:DetectModerationLabels
                  - rekognition:DetectText
                  - rekognition:DetectFaces
                Resource: '*'

  # Lambda function for file processing
  FileProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${EnvironmentName}-file-processor'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt FileProcessorRole.Arn
      Timeout: 300
      MemorySize: 1024
      ReservedConcurrencyLimit: 10
      Environment:
        Variables:
          UPLOAD_BUCKET: !Ref UploadBucket
          PROCESSED_BUCKET: !Ref ProcessedBucket
          METADATA_TABLE: !Ref FileMetadataTable
          SNS_TOPIC: !Ref FileProcessingTopic
          MAX_FILE_SIZE: !Ref MaxFileSize
          ALLOWED_FILE_TYPES: !Join [',', !Ref AllowedFileTypes]
          THUMBNAIL_SIZES: !Join [',', !Ref ThumbnailSizes]
          ENABLE_IMAGE_PROCESSING: !Ref EnableImageProcessing
          ENABLE_VIRUS_SCAN: !Ref EnableVirusScan
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import uuid
          from datetime import datetime, timedelta
          from urllib.parse import unquote_plus
          import re
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          sns = boto3.client('sns')
          rekognition = boto3.client('rekognition')
          
          def lambda_handler(event, context):
              print(f"File processor received: {json.dumps(event)}")
              
              metadata_table = dynamodb.Table(os.environ['METADATA_TABLE'])
              processed_bucket = os.environ['PROCESSED_BUCKET']
              sns_topic = os.environ['SNS_TOPIC']
              
              try:
                  for record in event['Records']:
                      if record['eventSource'] != 'aws:s3':
                          continue
                      
                      bucket = record['s3']['bucket']['name']
                      key = unquote_plus(record['s3']['object']['key'])
                      event_name = record['eventName']
                      
                      print(f"Processing {event_name} for {bucket}/{key}")
                      
                      if event_name.startswith('ObjectCreated'):
                          await process_upload(bucket, key, metadata_table, processed_bucket, sns_topic)
                      elif event_name.startswith('ObjectRemoved'):
                          await process_deletion(bucket, key, metadata_table)
                  
                  return {'statusCode': 200, 'body': 'Processing completed'}
                  
              except Exception as e:
                  print(f"Error processing file: {str(e)}")
                  sns.publish(
                      TopicArn=sns_topic,
                      Message=f"Error processing file: {str(e)}",
                      Subject="File Processing Error"
                  )
                  raise e
          
          async def process_upload(bucket, key, metadata_table, processed_bucket, sns_topic):
              file_id = str(uuid.uuid4())
              
              try:
                  # Get file metadata
                  response = s3.head_object(Bucket=bucket, Key=key)
                  file_size = response['ContentLength']
                  content_type = response.get('ContentType', 'application/octet-stream')
                  
                  # Validate file
                  validation_result = validate_file(key, file_size, content_type)
                  if not validation_result['valid']:
                      raise Exception(f"File validation failed: {validation_result['reason']}")
                  
                  # Extract file info
                  file_extension = key.split('.')[-1].lower() if '.' in key else ''
                  file_name = key.split('/')[-1]
                  
                  # Create metadata record
                  metadata = {
                      'fileId': file_id,
                      'originalKey': key,
                      'fileName': file_name,
                      'fileSize': file_size,
                      'contentType': content_type,
                      'fileExtension': file_extension,
                      'fileType': get_file_type(file_extension),
                      'status': 'processing',
                      'uploadedAt': datetime.utcnow().isoformat(),
                      'userId': extract_user_id_from_key(key),
                      'isPublic': False,
                      'tags': [],
                      'metadata': {}
                  }
                  
                  # Save initial metadata
                  metadata_table.put_item(Item=metadata)
                  
                  # Process based on file type
                  processing_results = {}
                  
                  if metadata['fileType'] == 'image' and os.environ.get('ENABLE_IMAGE_PROCESSING') == 'true':
                      processing_results = await process_image(bucket, key, processed_bucket, file_id)
                      
                      # Analyze image with Rekognition
                      try:
                          labels_response = rekognition.detect_labels(
                              Image={'S3Object': {'Bucket': bucket, 'Name': key}},
                              MaxLabels=10,
                              MinConfidence=80
                          )
                          metadata['metadata']['labels'] = [label['Name'] for label in labels_response['Labels']]
                          
                          # Check for inappropriate content
                          moderation_response = rekognition.detect_moderation_labels(
                              Image={'S3Object': {'Bucket': bucket, 'Name': key}},
                              MinConfidence=80
                          )
                          if moderation_response['ModerationLabels']:
                              metadata['status'] = 'flagged'
                              metadata['metadata']['moderation'] = [label['Name'] for label in moderation_response['ModerationLabels']]
                      except Exception as e:
                          print(f"Rekognition analysis failed: {str(e)}")
                  
                  elif metadata['fileType'] == 'document':
                      processing_results = await process_document(bucket, key, processed_bucket, file_id)
                  
                  # Update metadata with processing results
                  metadata['processedFiles'] = processing_results.get('files', {})
                  metadata['status'] = 'completed' if metadata['status'] != 'flagged' else 'flagged'
                  metadata['processedAt'] = datetime.utcnow().isoformat()
                  
                  # Update DynamoDB
                  metadata_table.put_item(Item=metadata)
                  
                  # Send notification
                  sns.publish(
                      TopicArn=sns_topic,
                      Message=json.dumps(metadata, default=str),
                      Subject=f"File processed: {file_name}"
                  )
                  
                  print(f"Successfully processed file: {file_id}")
                  
              except Exception as e:
                  # Update status to failed
                  metadata_table.update_item(
                      Key={'fileId': file_id},
                      UpdateExpression='SET #status = :status, errorMessage = :error',
                      ExpressionAttributeNames={'#status': 'status'},
                      ExpressionAttributeValues={':status': 'failed', ':error': str(e)}
                  )
                  raise e
          
          def validate_file(key, file_size, content_type):
              max_size = int(os.environ.get('MAX_FILE_SIZE', 10485760))
              allowed_types = os.environ.get('ALLOWED_FILE_TYPES', '').split(',')
              
              if file_size > max_size:
                  return {'valid': False, 'reason': f'File size {file_size} exceeds maximum {max_size}'}
              
              file_extension = key.split('.')[-1].lower() if '.' in key else ''
              if allowed_types and file_extension not in allowed_types:
                  return {'valid': False, 'reason': f'File type {file_extension} not allowed'}
              
              return {'valid': True}
          
          def get_file_type(extension):
              image_types = ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff', 'webp']
              document_types = ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'txt']
              video_types = ['mp4', 'avi', 'mov', 'wmv', 'flv', 'webm']
              audio_types = ['mp3', 'wav', 'flac', 'aac', 'ogg']
              
              if extension in image_types:
                  return 'image'
              elif extension in document_types:
                  return 'document'
              elif extension in video_types:
                  return 'video'
              elif extension in audio_types:
                  return 'audio'
              else:
                  return 'other'
          
          def extract_user_id_from_key(key):
              # Extract user ID from key path like "uploads/user123/filename.jpg"
              parts = key.split('/')
              if len(parts) >= 2 and parts[0] == 'uploads':
                  return parts[1]
              return 'anonymous'
          
          async def process_image(bucket, key, processed_bucket, file_id):
              # This would implement image processing using PIL or similar
              # For now, just copy the original
              try:
                  # Copy original to processed bucket
                  copy_source = {'Bucket': bucket, 'Key': key}
                  processed_key = f"images/{file_id}/original.{key.split('.')[-1]}"
                  
                  s3.copy_object(
                      CopySource=copy_source,
                      Bucket=processed_bucket,
                      Key=processed_key,
                      TaggingDirective='COPY'
                  )
                  
                  files = {'original': processed_key}
                  
                  # Generate thumbnails (simplified)
                  thumbnail_sizes = os.environ.get('THUMBNAIL_SIZES', '150x150,300x300').split(',')
                  for size in thumbnail_sizes:
                      try:
                          width, height = map(int, size.split('x'))
                          thumbnail_key = f"images/{file_id}/thumbnail_{width}x{height}.jpg"
                          
                          # In a real implementation, you would resize the image here
                          # For now, just copy the original
                          s3.copy_object(
                              CopySource=copy_source,
                              Bucket=processed_bucket,
                              Key=thumbnail_key
                          )
                          
                          files[f'thumbnail_{width}x{height}'] = thumbnail_key
                      except Exception as e:
                          print(f"Failed to create thumbnail {size}: {str(e)}")
                  
                  return {'files': files}
                  
              except Exception as e:
                  print(f"Image processing failed: {str(e)}")
                  return {'files': {}}
          
          async def process_document(bucket, key, processed_bucket, file_id):
              # Document processing logic would go here
              try:
                  copy_source = {'Bucket': bucket, 'Key': key}
                  processed_key = f"documents/{file_id}/original.{key.split('.')[-1]}"
                  
                  s3.copy_object(
                      CopySource=copy_source,
                      Bucket=processed_bucket,
                      Key=processed_key,
                      TaggingDirective='COPY'
                  )
                  
                  return {'files': {'original': processed_key}}
                  
              except Exception as e:
                  print(f"Document processing failed: {str(e)}")
                  return {'files': {}}
          
          async def process_deletion(bucket, key, metadata_table):
              # Find and clean up related records
              try:
                  # This is simplified - in practice you'd need to query by originalKey
                  print(f"Cleaning up records for deleted file: {key}")
              except Exception as e:
                  print(f"Cleanup failed: {str(e)}")
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # Lambda permission for S3 to invoke the function
  FileProcessorLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref FileProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !GetAtt UploadBucket.Arn

  # API Gateway for file upload
  FileUploadApi:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: !Sub '${ProjectName}-${EnvironmentName}-upload-api'
      Description: File upload API
      BinaryMediaTypes:
        - '*/*'
      EndpointConfiguration:
        Types:
          - REGIONAL
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # API Gateway Lambda function
  FileUploadApiFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${EnvironmentName}-upload-api'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt FileUploadApiRole.Arn
      Timeout: 30
      MemorySize: 256
      Environment:
        Variables:
          UPLOAD_BUCKET: !Ref UploadBucket
          METADATA_TABLE: !Ref FileMetadataTable
          MAX_FILE_SIZE: !Ref MaxFileSize
          ALLOWED_FILE_TYPES: !Join [',', !Ref AllowedFileTypes]
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import uuid
          from datetime import datetime, timedelta
          import base64
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          
          def lambda_handler(event, context):
              print(f"Upload API received: {json.dumps(event, default=str)}")
              
              try:
                  http_method = event['httpMethod']
                  path = event['path']
                  
                  if http_method == 'POST' and path == '/upload/presigned-url':
                      return generate_presigned_url(event)
                  elif http_method == 'POST' and path == '/upload/direct':
                      return handle_direct_upload(event)
                  elif http_method == 'GET' and path.startswith('/files/'):
                      return get_file_info(event)
                  elif http_method == 'DELETE' and path.startswith('/files/'):
                      return delete_file(event)
                  else:
                      return {
                          'statusCode': 404,
                          'headers': {'Content-Type': 'application/json'},
                          'body': json.dumps({'error': 'Endpoint not found'})
                      }
                      
              except Exception as e:
                  print(f"API error: {str(e)}")
                  return {
                      'statusCode': 500,
                      'headers': {'Content-Type': 'application/json'},
                      'body': json.dumps({'error': 'Internal server error'})
                  }
          
          def generate_presigned_url(event):
              body = json.loads(event.get('body', '{}'))
              
              filename = body.get('filename')
              content_type = body.get('contentType')
              user_id = body.get('userId', 'anonymous')
              
              if not filename:
                  return {
                      'statusCode': 400,
                      'headers': {'Content-Type': 'application/json'},
                      'body': json.dumps({'error': 'Filename is required'})
                  }
              
              # Generate unique key
              file_id = str(uuid.uuid4())
              key = f"uploads/{user_id}/{file_id}-{filename}"
              
              # Generate presigned URL
              presigned_url = s3.generate_presigned_url(
                  'put_object',
                  Params={
                      'Bucket': os.environ['UPLOAD_BUCKET'],
                      'Key': key,
                      'ContentType': content_type
                  },
                  ExpiresIn=3600  # 1 hour
              )
              
              return {
                  'statusCode': 200,
                  'headers': {
                      'Content-Type': 'application/json',
                      'Access-Control-Allow-Origin': '*'
                  },
                  'body': json.dumps({
                      'uploadUrl': presigned_url,
                      'key': key,
                      'fileId': file_id
                  })
              }
          
          def handle_direct_upload(event):
              # Handle direct file upload through API Gateway
              # This is simplified - binary data handling in API Gateway is complex
              return {
                  'statusCode': 501,
                  'headers': {'Content-Type': 'application/json'},
                  'body': json.dumps({'error': 'Direct upload not implemented - use presigned URL'})
              }
          
          def get_file_info(event):
              file_id = event['pathParameters']['fileId']
              
              table = dynamodb.Table(os.environ['METADATA_TABLE'])
              
              try:
                  response = table.get_item(Key={'fileId': file_id})
                  
                  if 'Item' not in response:
                      return {
                          'statusCode': 404,
                          'headers': {'Content-Type': 'application/json'},
                          'body': json.dumps({'error': 'File not found'})
                      }
                  
                  item = response['Item']
                  
                  return {
                      'statusCode': 200,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps(item, default=str)
                  }
                  
              except Exception as e:
                  print(f"Error getting file info: {str(e)}")
                  return {
                      'statusCode': 500,
                      'headers': {'Content-Type': 'application/json'},
                      'body': json.dumps({'error': 'Internal server error'})
                  }
          
          def delete_file(event):
              file_id = event['pathParameters']['fileId']
              
              table = dynamodb.Table(os.environ['METADATA_TABLE'])
              
              try:
                  # Get file metadata
                  response = table.get_item(Key={'fileId': file_id})
                  
                  if 'Item' not in response:
                      return {
                          'statusCode': 404,
                          'headers': {'Content-Type': 'application/json'},
                          'body': json.dumps({'error': 'File not found'})
                      }
                  
                  item = response['Item']
                  
                  # Delete from S3
                  s3.delete_object(
                      Bucket=os.environ['UPLOAD_BUCKET'],
                      Key=item['originalKey']
                  )
                  
                  # Delete metadata
                  table.delete_item(Key={'fileId': file_id})
                  
                  return {
                      'statusCode': 200,
                      'headers': {
                          'Content-Type': 'application/json',
                          'Access-Control-Allow-Origin': '*'
                      },
                      'body': json.dumps({'message': 'File deleted successfully'})
                  }
                  
              except Exception as e:
                  print(f"Error deleting file: {str(e)}")
                  return {
                      'statusCode': 500,
                      'headers': {'Content-Type': 'application/json'},
                      'body': json.dumps({'error': 'Internal server error'})
                  }
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # IAM Role for Upload API Lambda
  FileUploadApiRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: FileUploadApiPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:DeleteObject
                Resource: !Sub '${UploadBucket}/*'
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:DeleteItem
                  - dynamodb:Query
                Resource:
                  - !GetAtt FileMetadataTable.Arn
                  - !Sub '${FileMetadataTable.Arn}/index/*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:GenerateDataKey
                Resource: !GetAtt FileUploadKMSKey.Arn

  # CloudFront Distribution for processed files
  ProcessedFilesDistribution:
    Type: AWS::CloudFront::Distribution
    Properties:
      DistributionConfig:
        DefaultCacheBehavior:
          TargetOriginId: ProcessedS3Origin
          ViewerProtocolPolicy: redirect-to-https
          CachePolicyId: 658327ea-f89d-4fab-a63d-7e88639e58f6  # Managed-CachingOptimized
          OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf  # Managed-CORS-S3Origin
        Origins:
          - Id: ProcessedS3Origin
            DomainName: !GetAtt ProcessedBucket.RegionalDomainName
            S3OriginConfig:
              OriginAccessIdentity: !Sub 'origin-access-identity/cloudfront/${ProcessedFilesOAI}'
        Enabled: true
        HttpVersion: http2
        IPV6Enabled: true
        PriceClass: PriceClass_100
        ViewerCertificate:
          CloudFrontDefaultCertificate: true
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # Origin Access Identity for CloudFront
  ProcessedFilesOAI:
    Type: AWS::CloudFront::OriginAccessIdentity
    Properties:
      OriginAccessIdentityConfig:
        Comment: !Sub 'OAI for ${ProjectName}-${EnvironmentName} processed files'

  # SNS Topic for file processing notifications
  FileProcessingTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${ProjectName}-${EnvironmentName}-file-processing'
      DisplayName: !Sub '${ProjectName} ${EnvironmentName} File Processing'
      KmsMasterKeyId: !Ref FileUploadKMSKey
      Tags:
        - Key: Environment
          Value: !Ref EnvironmentName
        - Key: Project
          Value: !Ref ProjectName

  # CloudWatch Alarms
  FileProcessorErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-${EnvironmentName}-file-processor-errors'
      AlarmDescription: File processor function errors
      MetricName: Errors
      Namespace: AWS/Lambda
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 2
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref FileProcessorFunction
      AlarmActions:
        - !Ref FileProcessingTopic

  UploadBucketSizeAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-${EnvironmentName}-upload-bucket-size'
      AlarmDescription: Upload bucket size monitoring
      MetricName: BucketSizeBytes
      Namespace: AWS/S3
      Statistic: Average
      Period: 86400  # Daily
      EvaluationPeriods: 1
      Threshold: 1073741824000  # 1TB
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: BucketName
          Value: !Ref UploadBucket
        - Name: StorageType
          Value: StandardStorage

Outputs:
  UploadBucketName:
    Description: Upload S3 Bucket Name
    Value: !Ref UploadBucket
    Export:
      Name: !Sub '${AWS::StackName}-UploadBucket'

  ProcessedBucketName:
    Description: Processed Files S3 Bucket Name
    Value: !Ref ProcessedBucket
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedBucket'

  FileMetadataTableName:
    Description: File Metadata DynamoDB Table Name
    Value: !Ref FileMetadataTable
    Export:
      Name: !Sub '${AWS::StackName}-FileMetadataTable'

  FileUploadApiUrl:
    Description: File Upload API URL
    Value: !Sub 'https://${FileUploadApi}.execute-api.${AWS::Region}.amazonaws.com/prod'
    Export:
      Name: !Sub '${AWS::StackName}-FileUploadApiUrl'

  ProcessedFilesCDNUrl:
    Description: CloudFront URL for processed files
    Value: !Sub 'https://${ProcessedFilesDistribution.DomainName}'
    Export:
      Name: !Sub '${AWS::StackName}-ProcessedFilesCDNUrl'

  FileProcessorFunctionArn:
    Description: File Processor Lambda Function ARN
    Value: !GetAtt FileProcessorFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-FileProcessorFunction'

  FileProcessingTopicArn:
    Description: File Processing SNS Topic ARN
    Value: !Ref FileProcessingTopic
    Export:
      Name: !Sub '${AWS::StackName}-FileProcessingTopic'

  FileUploadKMSKeyId:
    Description: File Upload KMS Key ID
    Value: !Ref FileUploadKMSKey
    Export:
      Name: !Sub '${AWS::StackName}-FileUploadKMSKey'