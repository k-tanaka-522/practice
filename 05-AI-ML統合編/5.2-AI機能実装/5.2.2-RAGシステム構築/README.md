# 5.2.2 RAG„Ç∑„Çπ„ÉÜ„É†ÊßãÁØâ

„Åì„ÅÆ„É¢„Ç∏„É•„Éº„É´„Åß„ÅØ„ÄÅRAGÔºàRetrieval-Augmented GenerationÔºâ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÁî®„ÅÑ„ÅüÈ´òÂ∫¶„Å™Áü•Ë≠ò„Éô„Éº„ÇπË≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†„ÇíÊßãÁØâ„Åó„Åæ„Åô„ÄÇÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„ÅÆÁü•Ë≠ò„ÇíÂ§ñÈÉ®„Éá„Éº„Çø„ÅßÊã°Âºµ„Åó„ÄÅÊ≠£Á¢∫„ÅßÊúÄÊñ∞„ÅÆÊÉÖÂ†±„Å´Âü∫„Å•„ÅèÂõûÁ≠î„ÇíÁîüÊàê„Åô„Çã„Ç∑„Çπ„ÉÜ„É†„ÇíÂÆüË£Ö„Åó„Åæ„Åô„ÄÇ

## üìã ÂâçÊèêÊù°‰ª∂

### ÂøÖÈ†à„ÅÆÂÆå‰∫Ü„É¢„Ç∏„É•„Éº„É´
- 1.1.1 AWS„Ç¢„Ç´„Ç¶„É≥„ÉàË®≠ÂÆö„Å®IAM
- 5.1.1 Bedrock„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó
- 5.1.2 „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàêÂÆüË£Ö

### ÊäÄË°ìË¶Å‰ª∂
- Python 3.9‰ª•‰∏ä„Åæ„Åü„ÅØNode.js 18‰ª•‰∏ä
- AWS CLI v2.0‰ª•‰∏ä
- DockerÔºà„É≠„Éº„Ç´„É´ÈñãÁô∫Áî®Ôºâ
- „Éô„ÇØ„Éà„É´Ê§úÁ¥¢„ÅÆÂü∫Êú¨ÁêÜËß£

### Ê®©ÈôêË¶Å‰ª∂
- BedrockÂü∫Áõ§„É¢„Éá„É´„Å∏„ÅÆ„Ç¢„ÇØ„Çª„ÇπÊ®©Èôê
- OpenSearch Service „ÅÆÁÆ°ÁêÜÊ®©Èôê
- LambdaÈñ¢Êï∞„ÅÆ‰ΩúÊàê„ÉªÂÆüË°åÊ®©Èôê
- S3„Éê„Ç±„ÉÉ„Éà„ÅÆË™≠„ÅøÊõ∏„ÅçÊ®©Èôê

## üéØ Â≠¶ÁøíÁõÆÊ®ô

„Åì„ÅÆ„É¢„Ç∏„É•„Éº„É´„ÇíÂÆå‰∫Ü„Åô„Çã„Å®„ÄÅ‰ª•‰∏ã„ÅÆ„Åì„Å®„Åå„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„ÅôÔºö

1. **RAG„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊ∑±„ÅÑÁêÜËß£**
   - Ê§úÁ¥¢Êã°ÂºµÁîüÊàê„ÅÆÂéüÁêÜ„Å®ÂÆüË£ÖÊñπÊ≥ï
   - „Éô„ÇØ„Éà„É´Âüã„ÇÅËæº„Åø„Å®„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢
   - ÊÉÖÂ†±Ê§úÁ¥¢„Å®ÁîüÊàê„ÅÆÁµ±ÂêàÊâãÊ≥ï

2. **È´òÊÄßËÉΩ„Å™ÊñáÊõ∏Âá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥ÊßãÁØâ**
   - Â§öÊßò„Å™ÂΩ¢Âºè„ÅÆÊñáÊõ∏Âá¶ÁêÜÔºàPDF„ÄÅWord„ÄÅHTMLÁ≠âÔºâ
   - „ÉÜ„Ç≠„Çπ„Éà„ÅÆÂàÜÂâ≤„ÉªÊ≠£Ë¶èÂåñ„Éª„ÇØ„É™„Éº„Éã„É≥„Ç∞
   - „É°„Çø„Éá„Éº„ÇøÊäΩÂá∫„Å®ÊßãÈÄ†Âåñ

3. **„Éô„ÇØ„Éà„É´Ê§úÁ¥¢„Ç∑„Çπ„ÉÜ„É†„ÅÆÂÆüË£Ö**
   - Amazon OpenSearch Service„ÅÆÊ¥ªÁî®
   - Âüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÅÆÁîüÊàê„Éª‰øùÂ≠ò„ÉªÊ§úÁ¥¢
   - „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢„Å®„Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ

4. **„Ç®„É≥„Çø„Éº„Éó„É©„Ç§„Ç∫„É¨„Éô„É´„ÅÆË≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†**
   - Ë§áÈõë„Å™„ÇØ„Ç®„É™„Å´ÂØæ„Åô„ÇãÊÆµÈöéÁöÑÊé®Ë´ñ
   - ÂõûÁ≠î„ÅÆ‰ø°È†ºÊÄß„Å®„ÇΩ„Éº„ÇπËøΩË∑°
   - ÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„ÅüÂØæË©±Ê©üËÉΩ

5. **„Çπ„Ç±„Éº„É©„Éñ„É´„Å™„Éá„Éº„ÇøÁÆ°ÁêÜ**
   - Â§ßË¶èÊ®°ÊñáÊõ∏„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥„ÅÆÂäπÁéáÁöÑ„Å™ÁÆ°ÁêÜ
   - „Ç§„É≥„ÇØ„É™„É°„É≥„Çø„É´Êõ¥Êñ∞„Å®„Éê„Éº„Ç∏„Éß„É≥ÁÆ°ÁêÜ
   - ÂàÜÊï£Âá¶ÁêÜ„Å´„Çà„ÇãÈ´òÈÄüÂåñ

6. **È´òÂ∫¶„Å™Ê§úÁ¥¢Á≤æÂ∫¶„ÅÆÊúÄÈÅ©Âåñ**
   - „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢Êà¶Áï•„ÅÆÂÆüË£Ö
   - „É™„É©„É≥„Ç≠„É≥„Ç∞„Å®ÁµêÊûú„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
   - „É¶„Éº„Ç∂„Éº„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„Å´„Çà„ÇãÁ∂ôÁ∂öÁöÑÊîπÂñÑ

7. **ÈÅãÁî®Áõ£Ë¶ñ„Å®ÂìÅË≥™ÁÆ°ÁêÜ**
   - Ê§úÁ¥¢„ÉªÁîüÊàêÂìÅË≥™„ÅÆ„É°„Éà„É™„ÇØ„ÇπË®àÊ∏¨
   - „É¶„Éº„Ç∂„ÉºÊ∫ÄË∂≥Â∫¶„ÅÆËøΩË∑°
   - „Ç∑„Çπ„ÉÜ„É†„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÁõ£Ë¶ñ

8. **„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Å®„Éó„É©„Ç§„Éê„Ç∑„ÉºÂØæÂøú**
   - „Ç¢„ÇØ„Çª„ÇπÂà∂Âæ°„Å®Ë™çË®ºË™çÂèØ
   - „Éá„Éº„Çø„ÅÆÊöóÂè∑Âåñ„Å®ÂåøÂêçÂåñ
   - Ê≥ïÁöÑ„Ç≥„É≥„Éó„É©„Ç§„Ç¢„É≥„ÇπÂØæÂøú

## üìê „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£Ê¶ÇË¶Å

### RAG„Ç∑„Çπ„ÉÜ„É†ÂÖ®‰ΩìÊßãÊàê

```mermaid
graph TB
    subgraph "Data Ingestion Layer"
        A[Document Sources]
        B[S3 Data Lake]
        C[Document Processor Lambda]
        D[Text Splitter]
        E[Metadata Extractor]
    end

    subgraph "Embedding & Index Layer"
        F[Bedrock Embeddings]
        G[OpenSearch Service]
        H[Vector Store]
        I[Metadata Index]
    end

    subgraph "Search & Retrieval Layer"
        J[Query Processor]
        K[Semantic Search]
        L[Hybrid Search]
        M[Re-ranker]
        N[Context Builder]
    end

    subgraph "Generation Layer"
        O[Bedrock LLM]
        P[Prompt Builder]
        Q[Response Generator]
        R[Citation Manager]
    end

    subgraph "Application Layer"
        S[API Gateway]
        T[Web Interface]
        U[Chat Interface]
        V[Analytics Dashboard]
    end

    subgraph "Monitoring & Management"
        W[CloudWatch]
        X[Performance Metrics]
        Y[Quality Metrics]
        Z[User Feedback]
    end

    A --> B
    B --> C
    C --> D
    C --> E
    D --> F
    F --> G
    F --> H
    E --> I
    
    S --> J
    J --> K
    J --> L
    K --> M
    L --> M
    M --> N
    N --> P
    P --> O
    O --> Q
    Q --> R
    
    S --> T
    S --> U
    T --> V
    
    Q --> W
    K --> X
    Q --> Y
    U --> Z
```

### ÊñáÊõ∏Âá¶ÁêÜ„Éï„É≠„Éº

```mermaid
sequenceDiagram
    participant User
    participant S3 as S3 Bucket
    participant Processor as Document Processor
    participant Splitter as Text Splitter
    participant Embedder as Bedrock Embeddings
    participant OpenSearch as OpenSearch Service
    participant Metadata as Metadata Store

    User->>S3: Upload Document
    S3->>Processor: Trigger Lambda
    Processor->>Processor: Extract Text & Metadata
    Processor->>Splitter: Send Processed Text
    Splitter->>Splitter: Split into Chunks
    Splitter->>Embedder: Generate Embeddings
    Embedder-->>Splitter: Return Vector Embeddings
    Splitter->>OpenSearch: Store Vectors & Text
    Splitter->>Metadata: Store Document Metadata
    OpenSearch-->>User: Index Ready for Search
```

### Ê§úÁ¥¢„ÉªÂõûÁ≠îÁîüÊàê„Éï„É≠„Éº

```mermaid
sequenceDiagram
    participant User
    participant API as API Gateway
    participant Query as Query Processor
    participant Embedder as Bedrock Embeddings
    participant Search as Search Engine
    participant Ranker as Re-ranker
    participant LLM as Bedrock LLM
    participant Response as Response Builder

    User->>API: Submit Question
    API->>Query: Process Query
    Query->>Embedder: Generate Query Embedding
    Embedder-->>Query: Return Query Vector
    Query->>Search: Semantic + Keyword Search
    Search-->>Query: Return Candidate Results
    Query->>Ranker: Re-rank Results
    Ranker-->>Query: Return Ranked Context
    Query->>LLM: Generate Answer with Context
    LLM-->>Query: Return Generated Answer
    Query->>Response: Build Final Response
    Response-->>User: Return Answer with Citations
```

## üõ† „Éè„É≥„Ç∫„Ç™„É≥ÂÆüË£Ö

### „Çπ„ÉÜ„ÉÉ„Éó1: ÊñáÊõ∏Âá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆÊßãÁØâ

#### 1.1 ÊñáÊõ∏Âèñ„ÇäËæº„Åø„ÉªÂá¶ÁêÜLambdaÈñ¢Êï∞„ÅÆÂÆüË£Ö

```python
# lambda/document-processor/lambda_function.py
import json
import boto3
import os
import re
from typing import List, Dict, Any, Optional
from urllib.parse import unquote_plus
import fitz  # PyMuPDF for PDF processing
import docx  # python-docx for Word documents
from bs4 import BeautifulSoup  # For HTML processing
import nltk
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer

# AWS Services
s3_client = boto3.client('s3')
bedrock_runtime = boto3.client('bedrock-runtime')
opensearch_client = boto3.client('opensearchserverless')

# Environment Variables
OPENSEARCH_ENDPOINT = os.environ.get('OPENSEARCH_ENDPOINT')
EMBEDDINGS_MODEL_ID = os.environ.get('EMBEDDINGS_MODEL_ID', 'amazon.titan-embed-text-v1')
CHUNK_SIZE = int(os.environ.get('CHUNK_SIZE', '1000'))
CHUNK_OVERLAP = int(os.environ.get('CHUNK_OVERLAP', '200'))

def lambda_handler(event, context):
    """
    „É°„Ç§„É≥„ÅÆLambda „Éè„É≥„Éâ„É©„ÉºÈñ¢Êï∞
    S3„Ç§„Éô„É≥„Éà„Åã„ÇâÊñáÊõ∏„ÇíÂá¶ÁêÜ„Åó„ÄÅ„Éô„ÇØ„Éà„É´Âåñ„Åó„Å¶OpenSearch„Å´‰øùÂ≠ò
    """
    try:
        # S3„Ç§„Éô„É≥„Éà„Åã„ÇâÊÉÖÂ†±„ÇíÊäΩÂá∫
        for record in event['Records']:
            bucket = record['s3']['bucket']['name']
            key = unquote_plus(record['s3']['object']['key'])
            
            print(f"Processing document: s3://{bucket}/{key}")
            
            # ÊñáÊõ∏„ÇíÂá¶ÁêÜ
            result = process_document(bucket, key)
            
            if result['success']:
                print(f"Successfully processed document: {key}")
                print(f"Created {result['chunks_count']} chunks")
            else:
                print(f"Failed to process document: {key}, Error: {result['error']}")
        
        return {
            'statusCode': 200,
            'body': json.dumps('Document processing completed successfully')
        }
        
    except Exception as e:
        print(f"Error in lambda_handler: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error processing document: {str(e)}')
        }

def process_document(bucket: str, key: str) -> Dict[str, Any]:
    """
    Âçò‰∏ÄÊñáÊõ∏„ÅÆÂá¶ÁêÜ„É°„Ç§„É≥Èñ¢Êï∞
    """
    try:
        # S3„Åã„ÇâÊñáÊõ∏„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ
        response = s3_client.get_object(Bucket=bucket, Key=key)
        document_content = response['Body'].read()
        
        # ÊñáÊõ∏„ÅÆÁ®ÆÈ°û„ÇíÂà§ÂÆö
        file_extension = key.lower().split('.')[-1]
        
        # „É°„Çø„Éá„Éº„Çø„ÇíÊäΩÂá∫
        metadata = extract_metadata(bucket, key, response)
        
        # ÊñáÊõ∏„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
        text_content = extract_text_content(document_content, file_extension)
        
        if not text_content.strip():
            return {
                'success': False,
                'error': 'No extractable text found in document'
            }
        
        # „ÉÜ„Ç≠„Çπ„Éà„ÇíÂâçÂá¶ÁêÜ
        processed_text = preprocess_text(text_content)
        
        # „ÉÜ„Ç≠„Çπ„Éà„Çí„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤
        chunks = split_text_into_chunks(processed_text, metadata)
        
        # ÂêÑ„ÉÅ„É£„É≥„ÇØ„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÇíÁîüÊàê
        embedded_chunks = []
        for i, chunk in enumerate(chunks):
            try:
                embedding = generate_embedding(chunk['text'])
                chunk['embedding'] = embedding
                chunk['chunk_id'] = f"{metadata['document_id']}_chunk_{i}"
                embedded_chunks.append(chunk)
            except Exception as e:
                print(f"Error generating embedding for chunk {i}: {str(e)}")
                continue
        
        # OpenSearch„Å´‰øùÂ≠ò
        indexing_result = store_in_opensearch(embedded_chunks, metadata)
        
        return {
            'success': True,
            'chunks_count': len(embedded_chunks),
            'indexing_result': indexing_result
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def extract_metadata(bucket: str, key: str, s3_response: Dict) -> Dict[str, Any]:
    """
    ÊñáÊõ∏„É°„Çø„Éá„Éº„Çø„ÅÆÊäΩÂá∫
    """
    import hashlib
    from datetime import datetime
    
    # S3„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„É°„Çø„Éá„Éº„Çø
    last_modified = s3_response.get('LastModified', datetime.now())
    content_length = s3_response.get('ContentLength', 0)
    content_type = s3_response.get('ContentType', 'application/octet-stream')
    
    # ÊñáÊõ∏ID„ÅÆÁîüÊàêÔºà„Éè„ÉÉ„Ç∑„É•„Éô„Éº„ÇπÔºâ
    document_id = hashlib.md5(f"{bucket}/{key}".encode()).hexdigest()
    
    metadata = {
        'document_id': document_id,
        'source_bucket': bucket,
        'source_key': key,
        'file_name': key.split('/')[-1],
        'file_extension': key.lower().split('.')[-1] if '.' in key else '',
        'content_type': content_type,
        'file_size': content_length,
        'upload_timestamp': last_modified.isoformat(),
        'processing_timestamp': datetime.now().isoformat(),
        'tags': extract_tags_from_path(key)
    }
    
    return metadata

def extract_tags_from_path(key: str) -> List[str]:
    """
    „Éï„Ç°„Ç§„É´„Éë„Çπ„Åã„Çâ„Çø„Ç∞„ÇíÊäΩÂá∫
    """
    tags = []
    path_parts = key.split('/')
    
    # „Éë„Çπ„ÅÆÂêÑÈÉ®ÂàÜ„Çí„Çø„Ç∞„Å®„Åó„Å¶‰ΩøÁî®
    for part in path_parts[:-1]:  # „Éï„Ç°„Ç§„É´Âêç„ÇíÈô§„Åè
        if part and part != '.':
            tags.append(part.lower())
    
    return tags

def extract_text_content(content: bytes, file_extension: str) -> str:
    """
    „Éï„Ç°„Ç§„É´„Çø„Ç§„Éó„Å´Âøú„Åò„Åü„ÉÜ„Ç≠„Çπ„ÉàÊäΩÂá∫
    """
    try:
        if file_extension == 'pdf':
            return extract_text_from_pdf(content)
        elif file_extension in ['doc', 'docx']:
            return extract_text_from_docx(content)
        elif file_extension in ['html', 'htm']:
            return extract_text_from_html(content)
        elif file_extension in ['txt', 'md']:
            return content.decode('utf-8', errors='ignore')
        else:
            # „ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´„Å®„Åó„Å¶Ë©¶Ë°å
            return content.decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Error extracting text from {file_extension}: {str(e)}")
        return ""

def extract_text_from_pdf(content: bytes) -> str:
    """
    PDF„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
    """
    import io
    
    text_content = []
    
    try:
        # PyMuPDF„Çí‰ΩøÁî®„Åó„Å¶PDF„ÇíÂá¶ÁêÜ
        pdf_document = fitz.open(stream=content, filetype="pdf")
        
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            text = page.get_text()
            
            if text.strip():
                text_content.append(text)
        
        pdf_document.close()
        
    except Exception as e:
        print(f"Error processing PDF: {str(e)}")
        return ""
    
    return '\n\n'.join(text_content)

def extract_text_from_docx(content: bytes) -> str:
    """
    WordÊñáÊõ∏„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
    """
    import io
    
    try:
        doc = docx.Document(io.BytesIO(content))
        text_content = []
        
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text_content.append(paragraph.text)
        
        return '\n\n'.join(text_content)
        
    except Exception as e:
        print(f"Error processing DOCX: {str(e)}")
        return ""

def extract_text_from_html(content: bytes) -> str:
    """
    HTML„Åã„Çâ„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
    """
    try:
        soup = BeautifulSoup(content, 'html.parser')
        
        # „Çπ„ÇØ„É™„Éó„Éà„Å®„Çπ„Çø„Ç§„É´„ÇíÂâäÈô§
        for script in soup(["script", "style"]):
            script.decompose()
        
        # „ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫
        text = soup.get_text()
        
        # Ë°åÈñì„ÇíÊï¥ÁêÜ
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
        
    except Exception as e:
        print(f"Error processing HTML: {str(e)}")
        return ""

def preprocess_text(text: str) -> str:
    """
    „ÉÜ„Ç≠„Çπ„Éà„ÅÆÂâçÂá¶ÁêÜ„ÉªÊ≠£Ë¶èÂåñ
    """
    # ‰∏çË¶Å„Å™Á©∫ÁôΩÊñáÂ≠ó„ÇíÈô§Âéª
    text = re.sub(r'\s+', ' ', text)
    
    # ÁâπÊÆäÊñáÂ≠ó„ÅÆÊ≠£Ë¶èÂåñ
    text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\"\']', ' ', text)
    
    # ÈÄ£Á∂ö„Åô„ÇãÂè•Ë™≠ÁÇπ„ÇíÂçò‰∏Ä„Å´
    text = re.sub(r'([\.!?]){2,}', r'\1', text)
    
    return text.strip()

def split_text_into_chunks(text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    „ÉÜ„Ç≠„Çπ„Éà„ÇíÊÑèÂë≥ÁöÑ„Å™„ÉÅ„É£„É≥„ÇØ„Å´ÂàÜÂâ≤
    """
    chunks = []
    
    try:
        # ÊñáÂçò‰Ωç„ÅßÂàÜÂâ≤
        sentences = sent_tokenize(text)
        
        current_chunk = ""
        current_chunk_sentences = []
        
        for sentence in sentences:
            # „ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„ÇíË∂Ö„Åà„ÇãÂ†¥Âêà„ÅØÊñ∞„Åó„ÅÑ„ÉÅ„É£„É≥„ÇØ„ÇíÈñãÂßã
            if len(current_chunk) + len(sentence) > CHUNK_SIZE and current_chunk:
                # „Ç™„Éº„Éê„Éº„É©„ÉÉ„ÉóÂàÜ„ÇíËÄÉÊÖÆ
                overlap_sentences = current_chunk_sentences[-2:] if len(current_chunk_sentences) >= 2 else current_chunk_sentences
                
                chunks.append({
                    'text': current_chunk.strip(),
                    'sentence_count': len(current_chunk_sentences),
                    'char_count': len(current_chunk),
                    'metadata': metadata.copy()
                })
                
                # Êñ∞„Åó„ÅÑ„ÉÅ„É£„É≥„ÇØ„Çí„Ç™„Éº„Éê„Éº„É©„ÉÉ„ÉóÊñá„ÅßÈñãÂßã
                current_chunk = ' '.join(overlap_sentences) + ' ' if overlap_sentences else ''
                current_chunk_sentences = overlap_sentences.copy() if overlap_sentences else []
            
            current_chunk += sentence + ' '
            current_chunk_sentences.append(sentence)
        
        # ÊúÄÂæå„ÅÆ„ÉÅ„É£„É≥„ÇØ„ÇíËøΩÂä†
        if current_chunk.strip():
            chunks.append({
                'text': current_chunk.strip(),
                'sentence_count': len(current_chunk_sentences),
                'char_count': len(current_chunk),
                'metadata': metadata.copy()
            })
    
    except Exception as e:
        print(f"Error in text splitting: {str(e)}")
        # „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: Âõ∫ÂÆö„Çµ„Ç§„Ç∫„Åß„ÅÆÂàÜÂâ≤
        chunks = split_text_fixed_size(text, metadata)
    
    return chunks

def split_text_fixed_size(text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Âõ∫ÂÆö„Çµ„Ç§„Ç∫„Åß„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÂàÜÂâ≤Ôºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁî®Ôºâ
    """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + CHUNK_SIZE
        
        # „Ç™„Éº„Éê„Éº„É©„ÉÉ„Éó„ÇíËÄÉÊÖÆ
        if start > 0:
            start -= CHUNK_OVERLAP
        
        chunk_text = text[start:end]
        
        chunks.append({
            'text': chunk_text,
            'char_count': len(chunk_text),
            'metadata': metadata.copy()
        })
        
        start = end
    
    return chunks

def generate_embedding(text: str) -> List[float]:
    """
    Bedrock„Çí‰ΩøÁî®„Åó„Å¶„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÇíÁîüÊàê
    """
    try:
        # Titan Embeddings „É¢„Éá„É´Áî®„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„Éú„Éá„Ç£
        body = {
            "inputText": text
        }
        
        response = bedrock_runtime.invoke_model(
            modelId=EMBEDDINGS_MODEL_ID,
            body=json.dumps(body)
        )
        
        response_body = json.loads(response['body'].read())
        embedding = response_body.get('embedding', [])
        
        return embedding
        
    except Exception as e:
        print(f"Error generating embedding: {str(e)}")
        raise

def store_in_opensearch(chunks: List[Dict[str, Any]], document_metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    „ÉÅ„É£„É≥„ÇØ„ÇíOpenSearch„Å´‰øùÂ≠ò
    """
    try:
        from opensearchpy import OpenSearch, RequestsHttpConnection
        from aws_requests_auth.aws_auth import AWSRequestsAuth
        
        # OpenSearch„ÇØ„É©„Ç§„Ç¢„É≥„ÉàË®≠ÂÆö
        host = OPENSEARCH_ENDPOINT.replace('https://', '')
        auth = AWSRequestsAuth(aws_access_key=os.environ['AWS_ACCESS_KEY_ID'],
                              aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],
                              aws_token=os.environ.get('AWS_SESSION_TOKEN'),
                              aws_host=host,
                              aws_region=os.environ['AWS_REGION'],
                              aws_service='es')
        
        client = OpenSearch(
            hosts=[{'host': host, 'port': 443}],
            http_auth=auth,
            use_ssl=True,
            verify_certs=True,
            connection_class=RequestsHttpConnection
        )
        
        # „Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÂêç
        index_name = f"documents-{document_metadata['file_extension']}"
        
        # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ‰ΩúÊàê
        if not client.indices.exists(index=index_name):
            create_index(client, index_name)
        
        # „Éê„É´„ÇØ„Ç§„É≥„Çµ„Éº„ÉàÁî®„ÅÆ„Éá„Éº„ÇøÊ∫ñÂÇô
        bulk_data = []
        successful_inserts = 0
        
        for chunk in chunks:
            doc = {
                'chunk_id': chunk['chunk_id'],
                'document_id': document_metadata['document_id'],
                'text': chunk['text'],
                'embedding': chunk['embedding'],
                'metadata': {
                    **document_metadata,
                    'chunk_info': {
                        'char_count': chunk['char_count'],
                        'sentence_count': chunk.get('sentence_count', 0)
                    }
                },
                'timestamp': document_metadata['processing_timestamp']
            }
            
            bulk_data.append({
                "index": {
                    "_index": index_name,
                    "_id": chunk['chunk_id']
                }
            })
            bulk_data.append(doc)
        
        # „Éê„É´„ÇØ„Ç§„É≥„Çµ„Éº„ÉàÂÆüË°å
        if bulk_data:
            response = client.bulk(body=bulk_data)
            
            # „Ç®„É©„Éº„ÉÅ„Çß„ÉÉ„ÇØ
            if response['errors']:
                for item in response['items']:
                    if 'index' in item and 'error' in item['index']:
                        print(f"Indexing error: {item['index']['error']}")
                    else:
                        successful_inserts += 1
            else:
                successful_inserts = len(chunks)
        
        return {
            'index_name': index_name,
            'total_chunks': len(chunks),
            'successful_inserts': successful_inserts,
            'errors': successful_inserts < len(chunks)
        }
        
    except Exception as e:
        print(f"Error storing in OpenSearch: {str(e)}")
        raise

def create_index(client, index_name: str):
    """
    OpenSearch„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Çí‰ΩúÊàê
    """
    mapping = {
        "mappings": {
            "properties": {
                "chunk_id": {"type": "keyword"},
                "document_id": {"type": "keyword"},
                "text": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "embedding": {
                    "type": "knn_vector",
                    "dimension": 1536,  # Titan Embeddings „ÅÆ„Éô„ÇØ„Éà„É´Ê¨°ÂÖÉ
                    "method": {
                        "name": "hnsw",
                        "space_type": "cosinesimil",
                        "engine": "nmslib",
                        "parameters": {
                            "ef_construction": 128,
                            "m": 24
                        }
                    }
                },
                "metadata": {
                    "type": "object",
                    "properties": {
                        "file_name": {"type": "keyword"},
                        "file_extension": {"type": "keyword"},
                        "content_type": {"type": "keyword"},
                        "tags": {"type": "keyword"},
                        "file_size": {"type": "long"},
                        "upload_timestamp": {"type": "date"},
                        "processing_timestamp": {"type": "date"}
                    }
                },
                "timestamp": {"type": "date"}
            }
        },
        "settings": {
            "index": {
                "knn": True,
                "knn.algo_param.ef_search": 100
            }
        }
    }
    
    client.indices.create(index=index_name, body=mapping)
    print(f"Created index: {index_name}")
```

#### 1.2 Ê§úÁ¥¢„ÉªË≥™ÂïèÂøúÁ≠îLambdaÈñ¢Êï∞„ÅÆÂÆüË£Ö

```python
# lambda/rag-query-processor/lambda_function.py
import json
import boto3
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

# AWS Services
bedrock_runtime = boto3.client('bedrock-runtime')

# Environment Variables
OPENSEARCH_ENDPOINT = os.environ.get('OPENSEARCH_ENDPOINT')
EMBEDDINGS_MODEL_ID = os.environ.get('EMBEDDINGS_MODEL_ID', 'amazon.titan-embed-text-v1')
LLM_MODEL_ID = os.environ.get('LLM_MODEL_ID', 'anthropic.claude-3-haiku-20240307-v1:0')
MAX_CONTEXT_LENGTH = int(os.environ.get('MAX_CONTEXT_LENGTH', '4000'))
TOP_K_RESULTS = int(os.environ.get('TOP_K_RESULTS', '5'))

def lambda_handler(event, context):
    """
    RAGË≥™ÂïèÂøúÁ≠î„ÅÆ„É°„Ç§„É≥„Éè„É≥„Éâ„É©„Éº
    """
    try:
        # „É™„ÇØ„Ç®„Çπ„Éà„Éú„Éá„Ç£„ÅÆËß£Êûê
        if 'body' in event:
            body = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
        else:
            body = event
        
        # „Éë„É©„É°„Éº„ÇøÊäΩÂá∫
        query = body.get('query', '').strip()
        search_type = body.get('search_type', 'hybrid')  # semantic, keyword, hybrid
        max_results = min(body.get('max_results', TOP_K_RESULTS), 20)
        include_sources = body.get('include_sources', True)
        filters = body.get('filters', {})
        
        # ÂÖ•ÂäõÊ§úË®º
        if not query:
            return create_response(400, {
                'error': 'Query is required',
                'message': 'Please provide a valid query string'
            })
        
        if len(query) > 1000:
            return create_response(400, {
                'error': 'Query too long',
                'message': 'Query must be less than 1000 characters'
            })
        
        print(f"Processing query: {query[:100]}...")
        
        # RAGÂá¶ÁêÜ„ÅÆÂÆüË°å
        result = process_rag_query(
            query=query,
            search_type=search_type,
            max_results=max_results,
            include_sources=include_sources,
            filters=filters
        )
        
        return create_response(200, result)
        
    except Exception as e:
        print(f"Error in lambda_handler: {str(e)}")
        return create_response(500, {
            'error': 'Internal server error',
            'message': 'An unexpected error occurred while processing your query'
        })

def process_rag_query(query: str, search_type: str, max_results: int, 
                     include_sources: bool, filters: Dict[str, Any]) -> Dict[str, Any]:
    """
    RAG„ÇØ„Ç®„É™„ÅÆÂá¶ÁêÜ„É°„Ç§„É≥Èñ¢Êï∞
    """
    start_time = datetime.now()
    
    try:
        # 1. „ÇØ„Ç®„É™„ÅÆÂâçÂá¶ÁêÜ
        processed_query = preprocess_query(query)
        
        # 2. Èñ¢ÈÄ£ÊñáÊõ∏„ÅÆÊ§úÁ¥¢
        search_results = search_relevant_documents(
            query=processed_query,
            search_type=search_type,
            max_results=max_results,
            filters=filters
        )
        
        if not search_results:
            return {
                'answer': "Áî≥„ÅóË®≥„Åî„Åñ„ÅÑ„Åæ„Åõ„Çì„Åå„ÄÅ„ÅîË≥™Âïè„Å´Èñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„ÇíË¶ã„Å§„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇÂà•„ÅÆË°®Áèæ„ÅßË≥™Âïè„Åó„Å¶„ÅÑ„Åü„Å†„Åè„Åã„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ„Åß„ÅäÂ∞ã„Å≠„Åè„Å†„Åï„ÅÑ„ÄÇ",
                'sources': [],
                'confidence': 0.0,
                'search_results_count': 0,
                'processing_time_ms': int((datetime.now() - start_time).total_seconds() * 1000)
            }
        
        # 3. „Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊßãÁØâ
        context = build_context_from_results(search_results, MAX_CONTEXT_LENGTH)
        
        # 4. LLM„Å´„Çà„ÇãÂõûÁ≠îÁîüÊàê
        answer_result = generate_answer_with_llm(processed_query, context, search_results)
        
        # 5. „É¨„Çπ„Éù„É≥„Çπ„ÅÆÊßãÁØâ
        response = {
            'answer': answer_result['answer'],
            'confidence': calculate_confidence_score(search_results, answer_result),
            'search_results_count': len(search_results),
            'processing_time_ms': int((datetime.now() - start_time).total_seconds() * 1000)
        }
        
        if include_sources:
            response['sources'] = format_sources(search_results)
        
        # 6. ‰ΩøÁî®Áµ±Ë®à„ÅÆË®òÈå≤
        log_query_statistics(query, search_results, answer_result, response)
        
        return response
        
    except Exception as e:
        print(f"Error in process_rag_query: {str(e)}")
        raise

def preprocess_query(query: str) -> str:
    """
    „ÇØ„Ç®„É™„ÅÆÂâçÂá¶ÁêÜ„ÉªÊ≠£Ë¶èÂåñ
    """
    # Âü∫Êú¨ÁöÑ„Å™„ÇØ„É™„Éº„Éã„É≥„Ç∞
    processed = re.sub(r'\s+', ' ', query.strip())
    
    # ÁâπÊÆäÊñáÂ≠ó„ÅÆÂá¶ÁêÜ
    processed = re.sub(r'[^\w\s\?\.\,\!\-\(\)]', ' ', processed)
    
    return processed

def search_relevant_documents(query: str, search_type: str, max_results: int, 
                            filters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Èñ¢ÈÄ£ÊñáÊõ∏„ÅÆÊ§úÁ¥¢ÂÆüË°å
    """
    try:
        from opensearchpy import OpenSearch, RequestsHttpConnection
        from aws_requests_auth.aws_auth import AWSRequestsAuth
        
        # OpenSearch„ÇØ„É©„Ç§„Ç¢„É≥„ÉàË®≠ÂÆö
        host = OPENSEARCH_ENDPOINT.replace('https://', '')
        auth = AWSRequestsAuth(aws_access_key=os.environ['AWS_ACCESS_KEY_ID'],
                              aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],
                              aws_token=os.environ.get('AWS_SESSION_TOKEN'),
                              aws_host=host,
                              aws_region=os.environ['AWS_REGION'],
                              aws_service='es')
        
        client = OpenSearch(
            hosts=[{'host': host, 'port': 443}],
            http_auth=auth,
            use_ssl=True,
            verify_certs=True,
            connection_class=RequestsHttpConnection
        )
        
        if search_type == 'semantic':
            return semantic_search(client, query, max_results, filters)
        elif search_type == 'keyword':
            return keyword_search(client, query, max_results, filters)
        else:  # hybrid
            return hybrid_search(client, query, max_results, filters)
            
    except Exception as e:
        print(f"Error in search_relevant_documents: {str(e)}")
        return []

def semantic_search(client, query: str, max_results: int, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢Ôºà„Éô„ÇØ„Éà„É´Ê§úÁ¥¢Ôºâ
    """
    try:
        # „ÇØ„Ç®„É™„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„ÇíÁîüÊàê
        query_embedding = generate_embedding(query)
        
        # „Éô„ÇØ„Éà„É´Ê§úÁ¥¢„ÇØ„Ç®„É™
        search_query = {
            "size": max_results,
            "query": {
                "bool": {
                    "must": [
                        {
                            "knn": {
                                "embedding": {
                                    "vector": query_embedding,
                                    "k": max_results
                                }
                            }
                        }
                    ]
                }
            },
            "_source": ["text", "metadata", "chunk_id", "document_id"]
        }
        
        # „Éï„Ç£„É´„Çø„ÅÆËøΩÂä†
        if filters:
            search_query["query"]["bool"]["filter"] = build_filters(filters)
        
        # Ê§úÁ¥¢ÂÆüË°å
        response = client.search(
            index="documents-*",
            body=search_query
        )
        
        return format_search_results(response['hits']['hits'])
        
    except Exception as e:
        print(f"Error in semantic_search: {str(e)}")
        return []

def keyword_search(client, query: str, max_results: int, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    „Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢ÔºàÂÖ®ÊñáÊ§úÁ¥¢Ôºâ
    """
    try:
        search_query = {
            "size": max_results,
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["text^2", "metadata.file_name"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ]
                }
            },
            "_source": ["text", "metadata", "chunk_id", "document_id"]
        }
        
        # „Éï„Ç£„É´„Çø„ÅÆËøΩÂä†
        if filters:
            search_query["query"]["bool"]["filter"] = build_filters(filters)
        
        # Ê§úÁ¥¢ÂÆüË°å
        response = client.search(
            index="documents-*",
            body=search_query
        )
        
        return format_search_results(response['hits']['hits'])
        
    except Exception as e:
        print(f"Error in keyword_search: {str(e)}")
        return []

def hybrid_search(client, query: str, max_results: int, filters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    „Éè„Ç§„Éñ„É™„ÉÉ„ÉâÊ§úÁ¥¢Ôºà„Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØ + „Ç≠„Éº„ÉØ„Éº„ÉâÔºâ
    """
    try:
        # „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢ÁµêÊûú
        semantic_results = semantic_search(client, query, max_results, filters)
        
        # „Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢ÁµêÊûú
        keyword_results = keyword_search(client, query, max_results, filters)
        
        # ÁµêÊûú„ÅÆÁµ±Âêà„Å®„É™„É©„É≥„Ç≠„É≥„Ç∞
        combined_results = combine_and_rerank_results(
            semantic_results, keyword_results, query, max_results
        )
        
        return combined_results
        
    except Exception as e:
        print(f"Error in hybrid_search: {str(e)}")
        return []

def combine_and_rerank_results(semantic_results: List[Dict], keyword_results: List[Dict], 
                              query: str, max_results: int) -> List[Dict[str, Any]]:
    """
    Ê§úÁ¥¢ÁµêÊûú„ÅÆÁµ±Âêà„Å®„É™„É©„É≥„Ç≠„É≥„Ç∞
    """
    try:
        # ÁµêÊûú„ÇíÁµ±ÂêàÔºàÈáçË§áÊéíÈô§Ôºâ
        combined = {}
        
        # „Çª„Éû„É≥„ÉÜ„Ç£„ÉÉ„ÇØÊ§úÁ¥¢ÁµêÊûúÔºàÈáç„Åø: 0.7Ôºâ
        for i, result in enumerate(semantic_results):
            chunk_id = result['chunk_id']
            combined[chunk_id] = result
            combined[chunk_id]['semantic_score'] = result['score']
            combined[chunk_id]['semantic_rank'] = i + 1
            combined[chunk_id]['keyword_score'] = 0
            combined[chunk_id]['keyword_rank'] = max_results + 1
        
        # „Ç≠„Éº„ÉØ„Éº„ÉâÊ§úÁ¥¢ÁµêÊûúÔºàÈáç„Åø: 0.3Ôºâ
        for i, result in enumerate(keyword_results):
            chunk_id = result['chunk_id']
            if chunk_id in combined:
                combined[chunk_id]['keyword_score'] = result['score']
                combined[chunk_id]['keyword_rank'] = i + 1
            else:
                combined[chunk_id] = result
                combined[chunk_id]['semantic_score'] = 0
                combined[chunk_id]['semantic_rank'] = max_results + 1
                combined[chunk_id]['keyword_score'] = result['score']
                combined[chunk_id]['keyword_rank'] = i + 1
        
        # „Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó
        for chunk_id, result in combined.items():
            # Ê≠£Ë¶èÂåñ„Åï„Çå„Åü„É©„É≥„ÇØ„Çπ„Ç≥„Ç¢
            norm_semantic = 1.0 / result['semantic_rank'] if result['semantic_rank'] <= max_results else 0
            norm_keyword = 1.0 / result['keyword_rank'] if result['keyword_rank'] <= max_results else 0
            
            # „Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Çπ„Ç≥„Ç¢ÔºàÈáç„Åø‰ªò„ÅçÔºâ
            result['hybrid_score'] = (0.7 * norm_semantic) + (0.3 * norm_keyword)
        
        # „Éè„Ç§„Éñ„É™„ÉÉ„Éâ„Çπ„Ç≥„Ç¢„Åß„ÇΩ„Éº„Éà
        sorted_results = sorted(combined.values(), key=lambda x: x['hybrid_score'], reverse=True)
        
        return sorted_results[:max_results]
        
    except Exception as e:
        print(f"Error in combine_and_rerank_results: {str(e)}")
        return semantic_results[:max_results]

def build_filters(filters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Ê§úÁ¥¢„Éï„Ç£„É´„Çø„ÅÆÊßãÁØâ
    """
    filter_clauses = []
    
    if 'file_extension' in filters:
        filter_clauses.append({
            "term": {"metadata.file_extension": filters['file_extension']}
        })
    
    if 'tags' in filters:
        filter_clauses.append({
            "terms": {"metadata.tags": filters['tags']}
        })
    
    if 'date_range' in filters:
        date_range = filters['date_range']
        range_filter = {"range": {"metadata.upload_timestamp": {}}}
        
        if 'from' in date_range:
            range_filter["range"]["metadata.upload_timestamp"]["gte"] = date_range['from']
        if 'to' in date_range:
            range_filter["range"]["metadata.upload_timestamp"]["lte"] = date_range['to']
        
        filter_clauses.append(range_filter)
    
    return filter_clauses

def format_search_results(hits: List[Dict]) -> List[Dict[str, Any]]:
    """
    Ê§úÁ¥¢ÁµêÊûú„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà
    """
    results = []
    
    for hit in hits:
        result = {
            'chunk_id': hit['_source']['chunk_id'],
            'document_id': hit['_source']['document_id'],
            'text': hit['_source']['text'],
            'score': hit['_score'],
            'metadata': hit['_source']['metadata']
        }
        results.append(result)
    
    return results

def build_context_from_results(search_results: List[Dict], max_length: int) -> str:
    """
    Ê§úÁ¥¢ÁµêÊûú„Åã„Çâ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„ÇíÊßãÁØâ
    """
    context_parts = []
    current_length = 0
    
    for i, result in enumerate(search_results):
        text = result['text']
        source_info = f"[Source {i+1}: {result['metadata']['file_name']}]"
        
        part = f"{source_info}\n{text}\n"
        
        if current_length + len(part) > max_length:
            break
        
        context_parts.append(part)
        current_length += len(part)
    
    return "\n".join(context_parts)

def generate_answer_with_llm(query: str, context: str, search_results: List[Dict]) -> Dict[str, Any]:
    """
    LLM„Çí‰ΩøÁî®„Åó„ÅüÂõûÁ≠îÁîüÊàê
    """
    try:
        # „Éó„É≠„É≥„Éó„Éà„ÅÆÊßãÁØâ
        prompt = build_answer_prompt(query, context, search_results)
        
        # Claude API„ÅÆÂëº„Å≥Âá∫„Åó
        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1000,
            "temperature": 0.3,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        response = bedrock_runtime.invoke_model(
            modelId=LLM_MODEL_ID,
            body=json.dumps(body)
        )
        
        response_body = json.loads(response['body'].read())
        answer = response_body['content'][0]['text']
        
        return {
            'answer': answer.strip(),
            'prompt_tokens': response_body.get('usage', {}).get('input_tokens', 0),
            'completion_tokens': response_body.get('usage', {}).get('output_tokens', 0)
        }
        
    except Exception as e:
        print(f"Error in generate_answer_with_llm: {str(e)}")
        return {
            'answer': "Áî≥„ÅóË®≥„Åî„Åñ„ÅÑ„Åæ„Åõ„Çì„Åå„ÄÅÂõûÁ≠î„ÅÆÁîüÊàê‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü„ÄÇ„Åó„Å∞„Çâ„ÅèÂæÖ„Å£„Å¶„Åã„ÇâÂÜçÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ",
            'prompt_tokens': 0,
            'completion_tokens': 0
        }

def build_answer_prompt(query: str, context: str, search_results: List[Dict]) -> str:
    """
    ÂõûÁ≠îÁîüÊàêÁî®„Éó„É≠„É≥„Éó„Éà„ÅÆÊßãÁØâ
    """
    prompt = f"""„ÅÇ„Å™„Åü„ÅØË≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†„ÅÆAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇÊèê‰æõ„Åï„Çå„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´Ê≠£Á¢∫„ÅßÊúâÁî®„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

## ÂõûÁ≠îÊôÇ„ÅÆÊåáÈáù:
1. Êèê‰æõ„Åï„Çå„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±„ÅÆ„Åø„ÇíÂü∫„Å´ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ
2. ‰∏çÊòé„Å™ÁÇπ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅÊé®Ê∏¨„Åß„ÅØ„Å™„Åè„ÄåÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã„ÄçÊó®„Çí‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ
3. ÂõûÁ≠î„ÅØÂàÜ„Åã„Çä„ÇÑ„Åô„Åè„ÄÅÊßãÈÄ†Âåñ„Åï„Çå„ÅüÂΩ¢Âºè„ÅßÊèêÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ
4. ÂèØËÉΩ„Å™Â†¥Âêà„ÅØ„ÄÅÂÖ∑‰Ωì‰æã„ÇÑË©≥Á¥∞„ÇíÂê´„ÇÅ„Å¶Ë™¨Êòé„Åó„Å¶„Åè„Å†„Åï„ÅÑ
5. Ë§áÊï∞„ÅÆ„ÇΩ„Éº„Çπ„Åã„Çâ„ÅÆÊÉÖÂ†±„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ„Åù„Çå„Çâ„ÇíÁµ±Âêà„Åó„Å¶ÂåÖÊã¨ÁöÑ„Å™ÂõûÁ≠î„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ

## „Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±:
{context}

## „É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè:
{query}

## ÂõûÁ≠î:
‰∏äË®ò„ÅÆ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÊÉÖÂ†±„ÇíÂü∫„Å´„ÄÅË≥™Âïè„Å´ÂØæ„Åô„ÇãÊ≠£Á¢∫„ÅßË©≥Á¥∞„Å™ÂõûÁ≠î„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊÉÖÂ†±„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÇÑ„ÄÅ„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Å´Èñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Åù„ÅÆÊó®„ÇíÊòéÁ¢∫„Å´‰ºù„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

    return prompt

def calculate_confidence_score(search_results: List[Dict], answer_result: Dict) -> float:
    """
    ÂõûÁ≠î„ÅÆ‰ø°È†ºÂ∫¶„Çπ„Ç≥„Ç¢Ë®àÁÆó
    """
    if not search_results:
        return 0.0
    
    # Ê§úÁ¥¢ÁµêÊûú„ÅÆ„Çπ„Ç≥„Ç¢Âπ≥Âùá
    avg_search_score = sum(result['score'] for result in search_results) / len(search_results)
    
    # Ê§úÁ¥¢ÁµêÊûúÊï∞„Å´„Çà„ÇãË™øÊï¥
    result_count_factor = min(len(search_results) / 5.0, 1.0)  # 5‰ª∂‰ª•‰∏ä„ÅßÊúÄÂ§ß
    
    # „Éà„Éº„ÇØ„É≥Êï∞„Å´„Çà„ÇãË™øÊï¥ÔºàÈï∑„ÅÑÂõûÁ≠î„Åª„Å©‰ø°È†ºÂ∫¶„ÅåÈ´ò„ÅÑÂÇæÂêëÔºâ
    token_factor = min(answer_result.get('completion_tokens', 0) / 200.0, 1.0)
    
    # Á∑èÂêà„Çπ„Ç≥„Ç¢
    confidence = (avg_search_score * 0.5 + result_count_factor * 0.3 + token_factor * 0.2)
    
    return round(min(confidence, 1.0), 2)

def format_sources(search_results: List[Dict]) -> List[Dict[str, Any]]:
    """
    „ÇΩ„Éº„ÇπÊÉÖÂ†±„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà
    """
    sources = []
    
    for i, result in enumerate(search_results):
        source = {
            'id': i + 1,
            'document_id': result['document_id'],
            'file_name': result['metadata']['file_name'],
            'chunk_id': result['chunk_id'],
            'relevance_score': round(result['score'], 3),
            'snippet': result['text'][:200] + "..." if len(result['text']) > 200 else result['text']
        }
        
        # ËøΩÂä†„É°„Çø„Éá„Éº„Çø
        if 'file_size' in result['metadata']:
            source['file_size'] = result['metadata']['file_size']
        if 'upload_timestamp' in result['metadata']:
            source['upload_date'] = result['metadata']['upload_timestamp']
        if 'tags' in result['metadata']:
            source['tags'] = result['metadata']['tags']
        
        sources.append(source)
    
    return sources

def generate_embedding(text: str) -> List[float]:
    """
    „ÉÜ„Ç≠„Çπ„Éà„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´ÁîüÊàê
    """
    try:
        body = {
            "inputText": text
        }
        
        response = bedrock_runtime.invoke_model(
            modelId=EMBEDDINGS_MODEL_ID,
            body=json.dumps(body)
        )
        
        response_body = json.loads(response['body'].read())
        embedding = response_body.get('embedding', [])
        
        return embedding
        
    except Exception as e:
        print(f"Error generating embedding: {str(e)}")
        raise

def log_query_statistics(query: str, search_results: List[Dict], 
                        answer_result: Dict, response: Dict):
    """
    „ÇØ„Ç®„É™Áµ±Ë®à„ÅÆ„É≠„Ç∞Ë®òÈå≤
    """
    stats = {
        'timestamp': datetime.now().isoformat(),
        'query_length': len(query),
        'search_results_count': len(search_results),
        'average_search_score': sum(r['score'] for r in search_results) / len(search_results) if search_results else 0,
        'answer_length': len(answer_result.get('answer', '')),
        'confidence_score': response.get('confidence', 0),
        'processing_time_ms': response.get('processing_time_ms', 0),
        'prompt_tokens': answer_result.get('prompt_tokens', 0),
        'completion_tokens': answer_result.get('completion_tokens', 0)
    }
    
    print(f"Query Statistics: {json.dumps(stats)}")

def create_response(status_code: int, body: Dict[str, Any]) -> Dict[str, Any]:
    """
    HTTP „É¨„Çπ„Éù„É≥„Çπ„ÅÆ‰ΩúÊàê
    """
    return {
        'statusCode': status_code,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Content-Type,Authorization',
            'Access-Control-Allow-Methods': 'POST,OPTIONS'
        },
        'body': json.dumps(body, ensure_ascii=False, default=str)
    }
```

### „Çπ„ÉÜ„ÉÉ„Éó2: CloudFormation„Ç§„É≥„Éï„É©„Çπ„Éà„É©„ÇØ„ÉÅ„É£

#### 2.1 RAG„Ç∑„Çπ„ÉÜ„É†Áî®CloudFormation„ÉÜ„É≥„Éó„É¨„Éº„Éà

```yaml
# cloudformation/rag-system.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Comprehensive RAG System with Bedrock and OpenSearch'

Parameters:
  ProjectName:
    Type: String
    Default: 'rag-system'
    Description: 'Project name for resource naming'
  
  EnvironmentName:
    Type: String
    Default: 'dev'
    AllowedValues: [dev, staging, prod]
    Description: 'Environment name'
  
  OpenSearchInstanceType:
    Type: String
    Default: 't3.small.search'
    AllowedValues: [t3.small.search, t3.medium.search, m6g.large.search, m6g.xlarge.search]
    Description: 'OpenSearch instance type'
  
  OpenSearchInstanceCount:
    Type: Number
    Default: 1
    MinValue: 1
    MaxValue: 20
    Description: 'Number of OpenSearch instances'

Conditions:
  IsProduction: !Equals [!Ref EnvironmentName, 'prod']

Resources:
  # S3 Bucket for document storage
  DocumentsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-${EnvironmentName}-documents'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 30
          - Id: TransitionToIA
            Status: Enabled
            Transitions:
              - TransitionInDays: 30
                StorageClass: STANDARD_IA
              - TransitionInDays: 90
                StorageClass: GLACIER
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt DocumentProcessorFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .pdf
          - Event: s3:ObjectCreated:*
            Function: !GetAtt DocumentProcessorFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .docx
          - Event: s3:ObjectCreated:*
            Function: !GetAtt DocumentProcessorFunction.Arn
            Filter:
              S3Key:
                Rules:
                  - Name: suffix
                    Value: .txt

  # VPC for OpenSearch
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: 10.0.0.0/16
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${EnvironmentName}-vpc'

  # Private Subnets for OpenSearch
  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.1.0/24
      AvailabilityZone: !Select [0, !GetAZs '']
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${EnvironmentName}-private-subnet-1'

  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref VPC
      CidrBlock: 10.0.2.0/24
      AvailabilityZone: !Select [1, !GetAZs '']
      Tags:
        - Key: Name
          Value: !Sub '${ProjectName}-${EnvironmentName}-private-subnet-2'

  # Security Group for OpenSearch
  OpenSearchSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: 'Security group for OpenSearch cluster'
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          SourceSecurityGroupId: !Ref LambdaSecurityGroup
          Description: 'HTTPS access from Lambda'
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          SourceSecurityGroupId: !Ref LambdaSecurityGroup
          Description: 'HTTP access from Lambda'

  # Security Group for Lambda
  LambdaSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: 'Security group for Lambda functions'
      VpcId: !Ref VPC
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: 'All outbound traffic'

  # OpenSearch Domain
  OpenSearchDomain:
    Type: AWS::OpenSearch::Domain
    Properties:
      DomainName: !Sub '${ProjectName}-${EnvironmentName}-search'
      EngineVersion: 'OpenSearch_2.3'
      ClusterConfig:
        InstanceType: !Ref OpenSearchInstanceType
        InstanceCount: !Ref OpenSearchInstanceCount
        DedicatedMasterEnabled: !If [IsProduction, true, false]
        MasterInstanceType: !If [IsProduction, 'm6g.medium.search', !Ref 'AWS::NoValue']
        MasterInstanceCount: !If [IsProduction, 3, !Ref 'AWS::NoValue']
        ZoneAwarenessEnabled: !If [IsProduction, true, false]
        ZoneAwarenessConfig: !If 
          - IsProduction
          - AvailabilityZoneCount: 2
          - !Ref 'AWS::NoValue'
      EBSOptions:
        EBSEnabled: true
        VolumeType: gp3
        VolumeSize: !If [IsProduction, 100, 20]
        Iops: !If [IsProduction, 3000, 3000]
        Throughput: !If [IsProduction, 125, 125]
      VPCOptions:
        SecurityGroupIds:
          - !Ref OpenSearchSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !If [IsProduction, !Ref PrivateSubnet2, !Ref 'AWS::NoValue']
      EncryptionAtRestOptions:
        Enabled: true
      NodeToNodeEncryptionOptions:
        Enabled: true
      DomainEndpointOptions:
        EnforceHTTPS: true
        TLSSecurityPolicy: 'Policy-Min-TLS-1-2-2019-07'
      AccessPolicies:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: !GetAtt LambdaExecutionRole.Arn
            Action:
              - es:ESHttpGet
              - es:ESHttpPost
              - es:ESHttpPut
              - es:ESHttpDelete
              - es:ESHttpHead
            Resource: !Sub 'arn:aws:es:${AWS::Region}:${AWS::AccountId}:domain/${ProjectName}-${EnvironmentName}-search/*'
      LogPublishingOptions:
        SEARCH_SLOW_LOGS:
          CloudWatchLogsLogGroupArn: !GetAtt OpenSearchLogGroup.Arn
          Enabled: true
        INDEX_SLOW_LOGS:
          CloudWatchLogsLogGroupArn: !GetAtt OpenSearchLogGroup.Arn
          Enabled: true
        ES_APPLICATION_LOGS:
          CloudWatchLogsLogGroupArn: !GetAtt OpenSearchLogGroup.Arn
          Enabled: true

  # Lambda Execution Role
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${EnvironmentName}-lambda-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource: !Sub '${DocumentsBucket}/*'
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource: !Ref DocumentsBucket
        - PolicyName: BedrockAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:InvokeModel
                  - bedrock:InvokeModelWithResponseStream
                Resource:
                  - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'
                  - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-haiku-20240307-v1:0'
                  - !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'
        - PolicyName: OpenSearchAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - es:ESHttpGet
                  - es:ESHttpPost
                  - es:ESHttpPut
                  - es:ESHttpDelete
                  - es:ESHttpHead
                Resource: !GetAtt OpenSearchDomain.Arn
        - PolicyName: CloudWatchAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:DescribeLogGroups
                  - logs:DescribeLogStreams
                Resource: '*'

  # Document Processor Lambda Function
  DocumentProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${EnvironmentName}-document-processor'
      Runtime: python3.11
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !Ref ArtifactsBucket
        S3Key: 'lambda/document-processor.zip'
      Environment:
        Variables:
          OPENSEARCH_ENDPOINT: !GetAtt OpenSearchDomain.DomainEndpoint
          EMBEDDINGS_MODEL_ID: 'amazon.titan-embed-text-v1'
          CHUNK_SIZE: '1000'
          CHUNK_OVERLAP: '200'
      Timeout: 900
      MemorySize: 2048
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2
      DeadLetterQueue:
        TargetArn: !GetAtt DeadLetterQueue.Arn
      TracingConfig:
        Mode: Active

  # RAG Query Processor Lambda Function
  QueryProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${EnvironmentName}-query-processor'
      Runtime: python3.11
      Handler: lambda_function.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        S3Bucket: !Ref ArtifactsBucket
        S3Key: 'lambda/rag-query-processor.zip'
      Environment:
        Variables:
          OPENSEARCH_ENDPOINT: !GetAtt OpenSearchDomain.DomainEndpoint
          EMBEDDINGS_MODEL_ID: 'amazon.titan-embed-text-v1'
          LLM_MODEL_ID: 'anthropic.claude-3-haiku-20240307-v1:0'
          MAX_CONTEXT_LENGTH: '4000'
          TOP_K_RESULTS: '5'
      Timeout: 300
      MemorySize: 1024
      VpcConfig:
        SecurityGroupIds:
          - !Ref LambdaSecurityGroup
        SubnetIds:
          - !Ref PrivateSubnet1
          - !Ref PrivateSubnet2
      ReservedConcurrencyLimit: !If [IsProduction, 50, 10]

  # API Gateway
  RestApi:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: !Sub '${ProjectName}-${EnvironmentName}-rag-api'
      Description: 'RAG System API'
      EndpointConfiguration:
        Types:
          - REGIONAL
      BinaryMediaTypes:
        - 'application/pdf'
        - 'application/msword'
        - 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'

  # API Resources
  QueryResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      RestApiId: !Ref RestApi
      ParentId: !GetAtt RestApi.RootResourceId
      PathPart: 'query'

  UploadResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      RestApiId: !Ref RestApi
      ParentId: !GetAtt RestApi.RootResourceId
      PathPart: 'upload'

  # API Methods
  QueryMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      RestApiId: !Ref RestApi
      ResourceId: !Ref QueryResource
      HttpMethod: POST
      AuthorizationType: NONE
      Integration:
        Type: AWS_PROXY
        IntegrationHttpMethod: POST
        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${QueryProcessorFunction.Arn}/invocations'
      MethodResponses:
        - StatusCode: 200
          ResponseHeaders:
            Access-Control-Allow-Origin: true
        - StatusCode: 400
        - StatusCode: 500

  # Lambda Permissions
  DocumentProcessorS3Permission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DocumentProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: !Sub '${DocumentsBucket}/*'

  QueryProcessorApiPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref QueryProcessorFunction
      Action: lambda:InvokeFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub '${RestApi}/*/*'

  # CloudWatch Log Groups
  OpenSearchLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/opensearch/domains/${ProjectName}-${EnvironmentName}'
      RetentionInDays: !If [IsProduction, 30, 7]

  DocumentProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${ProjectName}-${EnvironmentName}-document-processor'
      RetentionInDays: !If [IsProduction, 30, 7]

  QueryProcessorLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${ProjectName}-${EnvironmentName}-query-processor'
      RetentionInDays: !If [IsProduction, 30, 7]

  # Dead Letter Queue
  DeadLetterQueue:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: !Sub '${ProjectName}-${EnvironmentName}-dlq'
      MessageRetentionPeriod: 1209600  # 14 days
      VisibilityTimeoutSeconds: 60

  # CloudWatch Dashboard
  RAGDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: !Sub '${ProjectName}-${EnvironmentName}-rag-dashboard'
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "properties": {
                "metrics": [
                  ["AWS/Lambda", "Invocations", "FunctionName", "${DocumentProcessorFunction}"],
                  [".", "Duration", ".", "."],
                  [".", "Errors", ".", "."],
                  [".", "Throttles", ".", "."]
                ],
                "period": 300,
                "stat": "Sum",
                "region": "${AWS::Region}",
                "title": "Document Processor Metrics"
              }
            },
            {
              "type": "metric",
              "properties": {
                "metrics": [
                  ["AWS/Lambda", "Invocations", "FunctionName", "${QueryProcessorFunction}"],
                  [".", "Duration", ".", "."],
                  [".", "Errors", ".", "."],
                  [".", "ConcurrentExecutions", ".", "."]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "Query Processor Metrics"
              }
            },
            {
              "type": "metric",
              "properties": {
                "metrics": [
                  ["AWS/ES", "SearchLatency", "DomainName", "${OpenSearchDomain}", "ClientId", "${AWS::AccountId}"],
                  [".", "IndexingLatency", ".", ".", ".", "."],
                  [".", "SearchRate", ".", ".", ".", "."],
                  [".", "IndexingRate", ".", ".", ".", "."]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "OpenSearch Performance"
              }
            }
          ]
        }

Outputs:
  DocumentsBucketName:
    Description: 'S3 bucket for document storage'
    Value: !Ref DocumentsBucket
    Export:
      Name: !Sub '${AWS::StackName}-DocumentsBucket'

  OpenSearchDomainEndpoint:
    Description: 'OpenSearch domain endpoint'
    Value: !GetAtt OpenSearchDomain.DomainEndpoint
    Export:
      Name: !Sub '${AWS::StackName}-OpenSearchEndpoint'

  ApiGatewayUrl:
    Description: 'API Gateway URL'
    Value: !Sub 'https://${RestApi}.execute-api.${AWS::Region}.amazonaws.com/${EnvironmentName}'
    Export:
      Name: !Sub '${AWS::StackName}-ApiUrl'

  QueryProcessorFunctionArn:
    Description: 'Query Processor Lambda Function ARN'
    Value: !GetAtt QueryProcessorFunction.Arn
    Export:
      Name: !Sub '${AWS::StackName}-QueryProcessorArn'
```

### „Çπ„ÉÜ„ÉÉ„Éó3: Âãï‰ΩúÁ¢∫Ë™ç„Å®„ÉÜ„Çπ„Éà

#### 3.1 Âü∫Êú¨Ê©üËÉΩ„ÉÜ„Çπ„Éà

```bash
#!/bin/bash
# scripts/test-rag-system.sh

set -e

PROJECT_NAME="rag-system"
ENVIRONMENT="dev"
REGION="us-east-1"

echo "Starting RAG System comprehensive testing..."

# Áí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆö
API_URL=$(aws cloudformation describe-stacks \
    --stack-name "${PROJECT_NAME}-${ENVIRONMENT}" \
    --query 'Stacks[0].Outputs[?OutputKey==`ApiGatewayUrl`].OutputValue' \
    --output text \
    --region ${REGION})

BUCKET_NAME=$(aws cloudformation describe-stacks \
    --stack-name "${PROJECT_NAME}-${ENVIRONMENT}" \
    --query 'Stacks[0].Outputs[?OutputKey==`DocumentsBucketName`].OutputValue' \
    --output text \
    --region ${REGION})

echo "API URL: ${API_URL}"
echo "Documents Bucket: ${BUCKET_NAME}"

# 1. „Ç∑„Çπ„ÉÜ„É†„ÅÆÂÅ•ÂÖ®ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
echo "=== 1. Health Check ==="
health_check() {
    response=$(curl -s -w "%{http_code}" -o /tmp/health.json "${API_URL}/query" \
        -X POST \
        -H "Content-Type: application/json" \
        -d '{"query": "test"}')
    
    if [ "$response" = "200" ] || [ "$response" = "400" ]; then
        echo "‚úì API endpoint is accessible"
    else
        echo "‚úó API endpoint returned status: $response"
        exit 1
    fi
}

health_check

# 2. „Çµ„É≥„Éó„É´ÊñáÊõ∏„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
echo "=== 2. Document Upload Test ==="
upload_test_documents() {
    # „ÉÜ„Çπ„ÉàÁî®PDFÊñáÊõ∏„Çí‰ΩúÊàê
    cat > /tmp/test-document.txt << 'EOF'
# AWS Lambda „Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„Ç¨„Ç§„Éâ

## Ê¶ÇË¶Å
AWS Lambda„ÅØ„ÄÅ„Çµ„Éº„Éê„Éº„É¨„Çπ„Ç≥„É≥„Éî„É•„Éº„ÉÜ„Ç£„É≥„Ç∞„Çµ„Éº„Éì„Çπ„Åß„Åô„ÄÇ„Ç≥„Éº„Éâ„ÇíÂÆüË°å„Åô„Çã„Åü„ÇÅ„ÅÆ„Çµ„Éº„Éê„Éº„ÅÆ„Éó„É≠„Éì„Ç∏„Éß„Éã„É≥„Ç∞„ÇÑÁÆ°ÁêÜ„ÇíË°å„ÅÜÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ

## „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÊúÄÈÅ©Âåñ
1. „É°„É¢„É™Ë®≠ÂÆö„ÅÆÊúÄÈÅ©Âåñ
   - Èñ¢Êï∞„ÅÆÂÆüË°åÊôÇÈñì„Å®„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÁõ£Ë¶ñ
   - ÈÅ©Âàá„Å™„É°„É¢„É™„Çµ„Ç§„Ç∫„ÇíË®≠ÂÆö„Åô„Çã„Åì„Å®„Åß„Ç≥„Çπ„Éà„Å®ÊÄßËÉΩ„ÅÆ„Éê„É©„É≥„Çπ„ÇíÂèñ„Çã

2. Cold Start „ÅÆÂâäÊ∏õ
   - „Éó„É≠„Éì„Ç∏„Éß„Éã„É≥„Ç∞Ê∏à„ÅøÂêåÊôÇÂÆüË°åÊï∞„ÅÆË®≠ÂÆö
   - Èñ¢Êï∞„ÅÆÂàùÊúüÂåñ„Ç≥„Éº„Éâ„ÅÆÊúÄÈÅ©Âåñ

3. Êé•Á∂ö„Éó„Éº„É´„ÅÆÂà©Áî®
   - „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂ö„ÅÆÂÜçÂà©Áî®
   - AWS SDK „ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅÆÂÜçÂà©Áî®

## „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ
- ÊúÄÂ∞èÊ®©Èôê„ÅÆÂéüÂâá„Å´Âæì„Å£„ÅüIAM„É≠„Éº„É´„ÅÆË®≠ÂÆö
- Áí∞Â¢ÉÂ§âÊï∞„Åß„ÅÆÊ©üÂØÜÊÉÖÂ†±ÁÆ°ÁêÜ
- VPC„ÅÆÈÅ©Âàá„Å™Ë®≠ÂÆö

## Áõ£Ë¶ñ„Å®„É≠„Ç∞
- CloudWatch Logs„ÇíÊ¥ªÁî®„Åó„Åü„É≠„Ç∞ÁÆ°ÁêÜ
- X-Ray„Çí‰ΩøÁî®„Åó„ÅüÂàÜÊï£„Éà„É¨„Éº„Ç∑„É≥„Ç∞
- „Ç´„Çπ„Çø„É†„É°„Éà„É™„ÇØ„Çπ„ÅÆ‰ΩúÊàê„Å®Áõ£Ë¶ñ
EOF

    # S3„Å´„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
    aws s3 cp /tmp/test-document.txt "s3://${BUCKET_NAME}/test-docs/lambda-guide.txt" \
        --region ${REGION}
    
    echo "‚úì Test document uploaded successfully"
    
    # Âá¶ÁêÜÂÆå‰∫Ü„Åæ„ÅßÂæÖÊ©ü
    echo "Waiting for document processing (30 seconds)..."
    sleep 30
}

upload_test_documents

# 3. Âü∫Êú¨ÁöÑ„Å™Ë≥™ÂïèÂøúÁ≠î„ÉÜ„Çπ„Éà
echo "=== 3. Basic Q&A Test ==="
basic_qa_test() {
    query="AWS Lambda„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ"
    
    echo "Query: ${query}"
    
    response=$(curl -s "${API_URL}/query" \
        -X POST \
        -H "Content-Type: application/json" \
        -d "{\"query\": \"${query}\", \"include_sources\": true}")
    
    # „É¨„Çπ„Éù„É≥„Çπ„ÅÆÁ¢∫Ë™ç
    echo "Response:"
    echo "$response" | jq '.'
    
    # ÊàêÂäüÂà§ÂÆö
    if echo "$response" | jq -e '.answer' > /dev/null; then
        echo "‚úì Basic Q&A test passed"
    else
        echo "‚úó Basic Q&A test failed"
        echo "$response"
        exit 1
    fi
}

basic_qa_test

# 4. Ê§úÁ¥¢„Çø„Ç§„ÉóÂà•„ÉÜ„Çπ„Éà
echo "=== 4. Search Type Tests ==="
search_type_test() {
    query="Lambda Cold Start"
    
    for search_type in "semantic" "keyword" "hybrid"; do
        echo "Testing ${search_type} search..."
        
        response=$(curl -s "${API_URL}/query" \
            -X POST \
            -H "Content-Type: application/json" \
            -d "{\"query\": \"${query}\", \"search_type\": \"${search_type}\", \"max_results\": 3}")
        
        if echo "$response" | jq -e '.answer' > /dev/null; then
            echo "‚úì ${search_type} search test passed"
        else
            echo "‚úó ${search_type} search test failed"
        fi
    done
}

search_type_test

# 5. „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà
echo "=== 5. Performance Test ==="
performance_test() {
    query="AWS Lambda„ÅÆ„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Å´„Å§„ÅÑ„Å¶"
    
    echo "Running 10 concurrent queries..."
    
    start_time=$(date +%s)
    
    for i in {1..10}; do
        (
            response=$(curl -s "${API_URL}/query" \
                -X POST \
                -H "Content-Type: application/json" \
                -d "{\"query\": \"${query}\"}")
            
            processing_time=$(echo "$response" | jq -r '.processing_time_ms // 0')
            echo "Query $i: ${processing_time}ms"
        ) &
    done
    
    wait
    
    end_time=$(date +%s)
    total_time=$((end_time - start_time))
    
    echo "‚úì 10 queries completed in ${total_time} seconds"
}

performance_test

# 6. „Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÉÜ„Çπ„Éà
echo "=== 6. Error Handling Test ==="
error_handling_test() {
    # Á©∫„ÅÆ„ÇØ„Ç®„É™
    echo "Testing empty query..."
    response=$(curl -s "${API_URL}/query" \
        -X POST \
        -H "Content-Type: application/json" \
        -d '{"query": ""}')
    
    if echo "$response" | jq -e '.error' > /dev/null; then
        echo "‚úì Empty query error handling works"
    else
        echo "‚úó Empty query error handling failed"
    fi
    
    # ÈùûÂ∏∏„Å´Èï∑„ÅÑ„ÇØ„Ç®„É™
    echo "Testing very long query..."
    long_query=$(python3 -c "print('test ' * 300)")
    
    response=$(curl -s "${API_URL}/query" \
        -X POST \
        -H "Content-Type: application/json" \
        -d "{\"query\": \"${long_query}\"}")
    
    if echo "$response" | jq -e '.error' > /dev/null; then
        echo "‚úì Long query error handling works"
    else
        echo "‚úó Long query error handling failed"
    fi
}

error_handling_test

# 7. „Éï„Ç£„É´„Çø„ÉºÊ©üËÉΩ„ÉÜ„Çπ„Éà
echo "=== 7. Filter Test ==="
filter_test() {
    query="Lambda"
    
    response=$(curl -s "${API_URL}/query" \
        -X POST \
        -H "Content-Type: application/json" \
        -d "{
            \"query\": \"${query}\",
            \"filters\": {
                \"file_extension\": \"txt\"
            },
            \"include_sources\": true
        }")
    
    if echo "$response" | jq -e '.sources' > /dev/null; then
        echo "‚úì Filter test passed"
        
        # „Éï„Ç£„É´„Çø„ÉºÁµêÊûú„ÅÆÁ¢∫Ë™ç
        source_count=$(echo "$response" | jq '.sources | length')
        echo "Found ${source_count} sources with txt filter"
    else
        echo "‚úó Filter test failed"
    fi
}

filter_test

echo "=== RAG System Testing Completed ==="
echo "All tests have been executed. Check the results above for any failures."

# „ÉÜ„Çπ„ÉàÁµêÊûú„ÅÆ„Çµ„Éû„É™„ÉºÁîüÊàê
generate_test_summary() {
    cat > /tmp/test-summary.json << EOF
{
    "test_timestamp": "$(date -Iseconds)",
    "api_url": "${API_URL}",
    "bucket_name": "${BUCKET_NAME}",
    "tests_executed": [
        "health_check",
        "document_upload",
        "basic_qa",
        "search_types",
        "performance",
        "error_handling",
        "filters"
    ],
    "test_environment": {
        "project": "${PROJECT_NAME}",
        "environment": "${ENVIRONMENT}",
        "region": "${REGION}"
    }
}
EOF
    
    echo "Test summary saved to /tmp/test-summary.json"
}

generate_test_summary
```

#### 3.2 Ë©≥Á¥∞„Å™Ê©üËÉΩÊ§úË®º

```python
# scripts/rag_validation.py
import requests
import json
import time
import concurrent.futures
from typing import Dict, List, Any
import statistics

class RAGSystemValidator:
    def __init__(self, api_url: str):
        self.api_url = api_url.rstrip('/')
        self.session = requests.Session()
        self.test_results = []
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """ÂåÖÊã¨ÁöÑ„Å™Ê§úË®º„ÅÆÂÆüË°å"""
        print("Starting comprehensive RAG system validation...")
        
        results = {
            'timestamp': time.time(),
            'tests': {}
        }
        
        # 1. Âü∫Êú¨Ê©üËÉΩ„ÉÜ„Çπ„Éà
        results['tests']['basic_functionality'] = self.test_basic_functionality()
        
        # 2. Ê§úÁ¥¢Á≤æÂ∫¶„ÉÜ„Çπ„Éà
        results['tests']['search_accuracy'] = self.test_search_accuracy()
        
        # 3. „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà
        results['tests']['performance'] = self.test_performance()
        
        # 4. Â†ÖÁâ¢ÊÄß„ÉÜ„Çπ„Éà
        results['tests']['robustness'] = self.test_robustness()
        
        # 5. „Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„ÉÜ„Çπ„Éà
        results['tests']['scalability'] = self.test_scalability()
        
        return results
    
    def test_basic_functionality(self) -> Dict[str, Any]:
        """Âü∫Êú¨Ê©üËÉΩ„ÅÆ„ÉÜ„Çπ„Éà"""
        print("Testing basic functionality...")
        
        test_cases = [
            {
                'name': 'simple_question',
                'query': 'AWS Lambda„Å®„ÅØ‰Ωï„Åß„Åô„ÅãÔºü',
                'expected_keywords': ['Lambda', '„Çµ„Éº„Éê„Éº„É¨„Çπ', 'AWS']
            },
            {
                'name': 'detailed_question',
                'query': 'LambdaÈñ¢Êï∞„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÅÆÊñπÊ≥ï„ÇíË©≥„Åó„ÅèÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ',
                'expected_keywords': ['„É°„É¢„É™', 'Cold Start', 'ÊúÄÈÅ©Âåñ']
            },
            {
                'name': 'security_question',
                'query': 'Lambda„ÅÆ„Çª„Ç≠„É•„É™„ÉÜ„Ç£„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„ÅØÔºü',
                'expected_keywords': ['IAM', 'VPC', '„Çª„Ç≠„É•„É™„ÉÜ„Ç£']
            }
        ]
        
        results = []
        
        for case in test_cases:
            print(f"  Testing: {case['name']}")
            
            response = self.query_rag_system(case['query'])
            
            if response and 'answer' in response:
                # „Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÂ≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
                answer = response['answer'].lower()
                keyword_score = sum(1 for keyword in case['expected_keywords'] 
                                  if keyword.lower() in answer) / len(case['expected_keywords'])
                
                result = {
                    'test_name': case['name'],
                    'success': True,
                    'keyword_score': keyword_score,
                    'response_length': len(response['answer']),
                    'confidence': response.get('confidence', 0),
                    'processing_time': response.get('processing_time_ms', 0)
                }
            else:
                result = {
                    'test_name': case['name'],
                    'success': False,
                    'error': 'No valid response received'
                }
            
            results.append(result)
        
        # Á∑èÂêàË©ï‰æ°
        success_rate = sum(1 for r in results if r.get('success', False)) / len(results)
        avg_keyword_score = statistics.mean([r.get('keyword_score', 0) for r in results if r.get('success', False)])
        
        return {
            'success_rate': success_rate,
            'average_keyword_score': avg_keyword_score,
            'test_cases': results
        }
    
    def test_search_accuracy(self) -> Dict[str, Any]:
        """Ê§úÁ¥¢Á≤æÂ∫¶„ÅÆ„ÉÜ„Çπ„Éà"""
        print("Testing search accuracy...")
        
        search_types = ['semantic', 'keyword', 'hybrid']
        test_queries = [
            'Lambda Cold StartÂïèÈ°å',
            '„É°„É¢„É™Ë®≠ÂÆö„ÅÆÊúÄÈÅ©Âåñ',
            'VPC„Å®„Çª„Ç≠„É•„É™„ÉÜ„Ç£Ë®≠ÂÆö'
        ]
        
        results = {}
        
        for search_type in search_types:
            type_results = []
            
            for query in test_queries:
                response = self.query_rag_system(query, search_type=search_type, include_sources=True)
                
                if response and 'sources' in response:
                    # Èñ¢ÈÄ£ÊÄß„Çπ„Ç≥„Ç¢„ÅÆÂàÜÊûê
                    source_scores = [source.get('relevance_score', 0) for source in response['sources']]
                    
                    result = {
                        'query': query,
                        'source_count': len(response['sources']),
                        'avg_relevance_score': statistics.mean(source_scores) if source_scores else 0,
                        'max_relevance_score': max(source_scores) if source_scores else 0,
                        'confidence': response.get('confidence', 0)
                    }
                else:
                    result = {
                        'query': query,
                        'source_count': 0,
                        'avg_relevance_score': 0,
                        'max_relevance_score': 0,
                        'confidence': 0
                    }
                
                type_results.append(result)
            
            results[search_type] = {
                'queries': type_results,
                'avg_confidence': statistics.mean([r['confidence'] for r in type_results]),
                'avg_source_count': statistics.mean([r['source_count'] for r in type_results]),
                'avg_relevance': statistics.mean([r['avg_relevance_score'] for r in type_results])
            }
        
        return results
    
    def test_performance(self) -> Dict[str, Any]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà"""
        print("Testing performance...")
        
        test_query = "AWS Lambda„ÅÆÂü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ"
        num_requests = 20
        
        response_times = []
        success_count = 0
        
        start_time = time.time()
        
        for i in range(num_requests):
            request_start = time.time()
            response = self.query_rag_system(test_query)
            request_end = time.time()
            
            if response and 'answer' in response:
                success_count += 1
                response_times.append((request_end - request_start) * 1000)  # ms
        
        total_time = time.time() - start_time
        
        return {
            'total_requests': num_requests,
            'successful_requests': success_count,
            'success_rate': success_count / num_requests,
            'total_time_seconds': total_time,
            'avg_response_time_ms': statistics.mean(response_times) if response_times else 0,
            'min_response_time_ms': min(response_times) if response_times else 0,
            'max_response_time_ms': max(response_times) if response_times else 0,
            'median_response_time_ms': statistics.median(response_times) if response_times else 0,
            'requests_per_second': num_requests / total_time
        }
    
    def test_robustness(self) -> Dict[str, Any]:
        """Â†ÖÁâ¢ÊÄß„ÉÜ„Çπ„Éà"""
        print("Testing robustness...")
        
        edge_cases = [
            {'name': 'empty_query', 'query': ''},
            {'name': 'very_short_query', 'query': 'a'},
            {'name': 'very_long_query', 'query': 'Lambda ' * 200},
            {'name': 'special_characters', 'query': 'Lambda@#$%^&*()[]{}'},
            {'name': 'non_english', 'query': '„Åì„Çì„Å´„Å°„ÅØLambdaÈñ¢Êï∞„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ'},
            {'name': 'code_snippet', 'query': 'def lambda_handler(event, context): return "Hello"'},
            {'name': 'numeric_query', 'query': '12345 67890'},
            {'name': 'mixed_content', 'query': 'Lambda 123 @#$ „Åì„Çì„Å´„Å°„ÅØ function()'}
        ]
        
        results = []
        
        for case in edge_cases:
            print(f"  Testing: {case['name']}")
            
            response = self.query_rag_system(case['query'])
            
            if case['name'] == 'empty_query':
                # Á©∫„ÇØ„Ç®„É™„ÅØ„Ç®„É©„Éº„Å´„Å™„Çã„Åπ„Åç
                success = response is None or 'error' in response
            elif case['name'] in ['very_long_query', 'special_characters']:
                # Èï∑„Åô„Åé„Çã„ÇØ„Ç®„É™„ÇÑÁâπÊÆäÊñáÂ≠ó„ÅØ„Ç®„É©„Éº„Åæ„Åü„ÅØÈÅ©Âàá„Å™Âá¶ÁêÜ
                success = response is None or 'error' in response or 'answer' in response
            else:
                # „Åù„ÅÆ‰ªñ„ÅØÈÅ©Âàá„Å™ÂõûÁ≠î„Åæ„Åü„ÅØÈÅ©Âàá„Å™„Ç®„É©„ÉºÂá¶ÁêÜ
                success = response is not None and ('answer' in response or 'error' in response)
            
            results.append({
                'test_name': case['name'],
                'query_length': len(case['query']),
                'success': success,
                'has_answer': response and 'answer' in response,
                'has_error': response and 'error' in response
            })
        
        success_rate = sum(1 for r in results if r['success']) / len(results)
        
        return {
            'success_rate': success_rate,
            'edge_cases': results
        }
    
    def test_scalability(self) -> Dict[str, Any]:
        """„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„ÉÜ„Çπ„Éà"""
        print("Testing scalability...")
        
        concurrency_levels = [1, 5, 10, 20]
        test_query = "LambdaÈñ¢Êï∞„ÅÆË®≠ÂÆöÊñπÊ≥ï"
        
        results = {}
        
        for concurrency in concurrency_levels:
            print(f"  Testing with {concurrency} concurrent requests...")
            
            start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [executor.submit(self.query_rag_system, test_query) 
                          for _ in range(concurrency)]
                
                responses = []
                for future in concurrent.futures.as_completed(futures):
                    try:
                        response = future.result(timeout=60)
                        responses.append(response)
                    except Exception as e:
                        print(f"Request failed: {e}")
                        responses.append(None)
            
            end_time = time.time()
            
            successful_responses = [r for r in responses if r and 'answer' in r]
            
            results[concurrency] = {
                'total_requests': concurrency,
                'successful_requests': len(successful_responses),
                'success_rate': len(successful_responses) / concurrency,
                'total_time_seconds': end_time - start_time,
                'requests_per_second': concurrency / (end_time - start_time)
            }
        
        return results
    
    def query_rag_system(self, query: str, search_type: str = 'hybrid', 
                        include_sources: bool = False) -> Dict[str, Any]:
        """RAG„Ç∑„Çπ„ÉÜ„É†„Å∏„ÅÆ„ÇØ„Ç®„É™ÂÆüË°å"""
        try:
            payload = {
                'query': query,
                'search_type': search_type,
                'include_sources': include_sources
            }
            
            response = self.session.post(
                f'{self.api_url}/query',
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {'error': f'HTTP {response.status_code}', 'details': response.text}
                
        except Exception as e:
            return {'error': str(e)}
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """„ÉÜ„Çπ„ÉàÁµêÊûú„É¨„Éù„Éº„Éà„ÅÆÁîüÊàê"""
        report = []
        report.append("=== RAG System Validation Report ===")
        report.append(f"Test Timestamp: {time.ctime(results['timestamp'])}")
        report.append("")
        
        # Âü∫Êú¨Ê©üËÉΩ„ÉÜ„Çπ„ÉàÁµêÊûú
        basic = results['tests']['basic_functionality']
        report.append("1. Basic Functionality Test")
        report.append(f"   Success Rate: {basic['success_rate']:.2%}")
        report.append(f"   Average Keyword Score: {basic['average_keyword_score']:.2f}")
        report.append("")
        
        # Ê§úÁ¥¢Á≤æÂ∫¶„ÉÜ„Çπ„ÉàÁµêÊûú
        accuracy = results['tests']['search_accuracy']
        report.append("2. Search Accuracy Test")
        for search_type, data in accuracy.items():
            report.append(f"   {search_type.capitalize()} Search:")
            report.append(f"     Average Confidence: {data['avg_confidence']:.2f}")
            report.append(f"     Average Source Count: {data['avg_source_count']:.1f}")
            report.append(f"     Average Relevance: {data['avg_relevance']:.3f}")
        report.append("")
        
        # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„ÉàÁµêÊûú
        perf = results['tests']['performance']
        report.append("3. Performance Test")
        report.append(f"   Success Rate: {perf['success_rate']:.2%}")
        report.append(f"   Average Response Time: {perf['avg_response_time_ms']:.1f}ms")
        report.append(f"   Requests per Second: {perf['requests_per_second']:.1f}")
        report.append("")
        
        # Â†ÖÁâ¢ÊÄß„ÉÜ„Çπ„ÉàÁµêÊûú
        robust = results['tests']['robustness']
        report.append("4. Robustness Test")
        report.append(f"   Success Rate: {robust['success_rate']:.2%}")
        report.append("")
        
        # „Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„ÉÜ„Çπ„ÉàÁµêÊûú
        scale = results['tests']['scalability']
        report.append("5. Scalability Test")
        for concurrency, data in scale.items():
            report.append(f"   {concurrency} concurrent requests:")
            report.append(f"     Success Rate: {data['success_rate']:.2%}")
            report.append(f"     Requests per Second: {data['requests_per_second']:.1f}")
        
        return "\n".join(report)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("Usage: python rag_validation.py <api_url>")
        sys.exit(1)
    
    api_url = sys.argv[1]
    validator = RAGSystemValidator(api_url)
    
    # ÂåÖÊã¨ÁöÑ„Å™Ê§úË®ºÂÆüË°å
    results = validator.run_comprehensive_validation()
    
    # „É¨„Éù„Éº„ÉàÁîüÊàê„Å®Âá∫Âäõ
    report = validator.generate_report(results)
    print(report)
    
    # ÁµêÊûú„ÇíJSON„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
    with open('/tmp/rag_validation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nDetailed results saved to /tmp/rag_validation_results.json")
```

## ‚úÖ Ê§úË®ºÊñπÊ≥ï

### 1. „Ç∑„Çπ„ÉÜ„É†ÂÅ•ÂÖ®ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ

```bash
# „Ç§„É≥„Éï„É©„Çπ„Éà„É©„ÇØ„ÉÅ„É£„ÅÆÁ¢∫Ë™ç
aws cloudformation describe-stacks \
    --stack-name rag-system-dev \
    --query 'Stacks[0].StackStatus'

# OpenSearch„ÇØ„É©„Çπ„Çø„Éº„ÅÆÁä∂ÊÖãÁ¢∫Ë™ç
aws opensearch describe-domain \
    --domain-name rag-system-dev-search

# LambdaÈñ¢Êï∞„ÅÆÁ¢∫Ë™ç
aws lambda list-functions \
    --query 'Functions[?starts_with(FunctionName, `rag-system-dev`)].FunctionName'
```

### 2. ÊñáÊõ∏Âá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥Ê§úË®º

```bash
# „ÉÜ„Çπ„ÉàÊñáÊõ∏„ÅÆ„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ
aws s3 cp sample-document.pdf s3://rag-system-dev-documents/test/

# Âá¶ÁêÜÁä∂Ê≥Å„ÅÆÁ¢∫Ë™ç
aws logs filter-log-events \
    --log-group-name /aws/lambda/rag-system-dev-document-processor \
    --start-time $(date -d '5 minutes ago' +%s)000

# OpenSearch„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆÁ¢∫Ë™ç
curl -X GET "https://opensearch-endpoint/_cat/indices?v"
```

### 3. Ë≥™ÂïèÂøúÁ≠îÊ©üËÉΩ„ÉÜ„Çπ„Éà

```bash
# Âü∫Êú¨ÁöÑ„Å™Ë≥™Âïè„ÉÜ„Çπ„Éà
curl -X POST https://api-url/dev/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "AWS„ÅÆ„Éô„Çπ„Éà„Éó„É©„ÇØ„ÉÜ„Ç£„Çπ„Å´„Å§„ÅÑ„Å¶Êïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ",
    "search_type": "hybrid",
    "include_sources": true
  }'

# Ë§áÈõë„Å™Ë≥™Âïè„ÉÜ„Çπ„Éà
curl -X POST https://api-url/dev/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "LambdaÈñ¢Êï∞„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÅßÊúÄ„ÇÇÈáçË¶Å„Å™Ë¶ÅÁ¥†„ÅØ‰Ωï„Åß„Åô„ÅãÔºüÂÖ∑‰ΩìÁöÑ„Å™Ë®≠ÂÆöÂÄ§„ÇÇÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ",
    "max_results": 10,
    "filters": {
      "file_extension": "pdf"
    }
  }'
```

## üîß „Éà„É©„Éñ„É´„Ç∑„É•„Éº„ÉÜ„Ç£„É≥„Ç∞

### „Çà„Åè„ÅÇ„ÇãÂïèÈ°å„Å®Ëß£Ê±∫Á≠ñ

#### 1. ÊñáÊõ∏Âá¶ÁêÜ„ÅÆÂ§±Êïó
**ÁóáÁä∂**: ÊñáÊõ∏„Åå„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åï„Çå„Å¶„ÇÇÊ§úÁ¥¢„Å´ÂèçÊò†„Åï„Çå„Å™„ÅÑ

**Ëß£Ê±∫Á≠ñ**:
```bash
# LambdaÈñ¢Êï∞„ÅÆ„É≠„Ç∞Á¢∫Ë™ç
aws logs filter-log-events \
    --log-group-name /aws/lambda/rag-system-dev-document-processor \
    --filter-pattern "ERROR"

# OpenSearch„Å∏„ÅÆÊé•Á∂öÁ¢∫Ë™ç
aws opensearch describe-domain-health \
    --domain-name rag-system-dev-search

# ÊâãÂãï„Åß„ÅÆÊñáÊõ∏ÂÜçÂá¶ÁêÜ
aws lambda invoke \
    --function-name rag-system-dev-document-processor \
    --payload '{"Records":[{"s3":{"bucket":{"name":"bucket-name"},"object":{"key":"document-key"}}}]}' \
    response.json
```

#### 2. Ê§úÁ¥¢Á≤æÂ∫¶„ÅÆ‰Ωé‰∏ã
**ÁóáÁä∂**: Èñ¢ÈÄ£ÊÄß„ÅÆ‰Ωé„ÅÑÁµêÊûú„ÅåËøî„Åï„Çå„Çã

**Ëß£Ê±∫Á≠ñ**:
```python
# Âüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅÆÂ§âÊõ¥„ÉÜ„Çπ„Éà
def test_embedding_models():
    models = [
        'amazon.titan-embed-text-v1',
        'amazon.titan-embed-text-v2'  # Êñ∞„Åó„ÅÑ„Éê„Éº„Ç∏„Éß„É≥„ÅåÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà
    ]
    
    for model in models:
        # ÂêÑ„É¢„Éá„É´„Åß„ÅÆÊ§úÁ¥¢ÁµêÊûú„ÇíÊØîËºÉ
        test_query_with_model(model)

# „ÉÅ„É£„É≥„ÇØ„Çµ„Ç§„Ç∫„ÅÆÊúÄÈÅ©Âåñ
def optimize_chunk_size():
    sizes = [500, 1000, 1500, 2000]
    for size in sizes:
        # ÂêÑ„Çµ„Ç§„Ç∫„Åß„ÅÆÊ§úÁ¥¢Á≤æÂ∫¶„ÇíÊ∏¨ÂÆö
        measure_search_accuracy(chunk_size=size)
```

#### 3. API „É¨„Çπ„Éù„É≥„ÇπÊôÇÈñì„ÅÆÈï∑ÊúüÂåñ
**ÁóáÁä∂**: Ë≥™ÂïèÂøúÁ≠î„ÅÆ„É¨„Çπ„Éù„É≥„ÇπÊôÇÈñì„ÅåÈÅÖ„ÅÑ

**Ëß£Ê±∫Á≠ñ**:
```yaml
# LambdaÈñ¢Êï∞„ÅÆ„É°„É¢„É™Â¢óÂä†
Properties:
  MemorySize: 2048  # 1024„Åã„ÇâÂ¢óÂä†
  
# OpenSearch„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÅÆ„Çπ„Ç±„Éº„É´„Ç¢„ÉÉ„Éó
Properties:
  ClusterConfig:
    InstanceType: m6g.large.search  # t3.small.search„Åã„ÇâÂ§âÊõ¥
    InstanceCount: 2
```

#### 4. OpenSearchÊé•Á∂ö„Ç®„É©„Éº
**ÁóáÁä∂**: LambdaÈñ¢Êï∞„Åã„ÇâOpenSearch„Å´Êé•Á∂ö„Åß„Åç„Å™„ÅÑ

**Ëß£Ê±∫Á≠ñ**:
```bash
# VPCË®≠ÂÆö„ÅÆÁ¢∫Ë™ç
aws ec2 describe-security-groups \
    --group-ids sg-xxxxxxxxx

# IAM„É≠„Éº„É´„ÅÆÊ®©ÈôêÁ¢∫Ë™ç
aws iam get-role-policy \
    --role-name rag-system-dev-lambda-role \
    --policy-name OpenSearchAccess

# „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÊé•Á∂ö„ÉÜ„Çπ„Éà
aws lambda invoke \
    --function-name rag-system-dev-query-processor \
    --payload '{"test": "connection"}' \
    test-response.json
```

## üìä „Ç≥„Çπ„ÉàË¶ãÁ©ç„ÇÇ„Çä

### ÈñãÁô∫Áí∞Â¢É„Åß„ÅÆÊúàÈ°ç„Ç≥„Çπ„ÉàÔºàÊé®ÂÆöÔºâ

| „Çµ„Éº„Éì„Çπ | ‰ΩøÁî®Èáè | Âçò‰æ° | ÊúàÈñì„Ç≥„Çπ„Éà |
|---------|--------|------|-----------| 
| OpenSearch (t3.small.search) | 1„Ç§„É≥„Çπ„Çø„É≥„Çπ | $24.48/Êúà | $24.48 |
| LambdaÂÆüË°å | 10,000ÂõûÂÆüË°å | $0.20/1M | $0.002 |
| Bedrock Embeddings | 100‰∏á„Éà„Éº„ÇØ„É≥ | $0.10/1M | $0.10 |
| Bedrock LLM (Claude Haiku) | 100‰∏á„Éà„Éº„ÇØ„É≥ | $0.25/1M | $0.25 |
| S3„Çπ„Éà„É¨„Éº„Ç∏ | 10GB | $0.023/GB | $0.23 |
| VPC (NAT Gateway) | 1„Ç≤„Éº„Éà„Ç¶„Çß„Ç§ | $32.40/Êúà | $32.40 |
| CloudWatch Logs | 1GB | $0.50/GB | $0.50 |

**ÈñãÁô∫Áí∞Â¢ÉÊúàÈ°çÁ∑èË®à: Á¥Ñ $57.98**

### Êú¨Áï™Áí∞Â¢ÉÔºà‰∏≠Ë¶èÊ®°Ôºâ„Åß„ÅÆÊúàÈ°ç„Ç≥„Çπ„ÉàÔºàÊé®ÂÆöÔºâ

| „Çµ„Éº„Éì„Çπ | ‰ΩøÁî®Èáè | Âçò‰æ° | ÊúàÈñì„Ç≥„Çπ„Éà |
|---------|--------|------|-----------| 
| OpenSearch (m6g.large.search √ó 2) | 2„Ç§„É≥„Çπ„Çø„É≥„Çπ | $145.44/Êúà | $290.88 |
| LambdaÂÆüË°å | 100‰∏áÂõûÂÆüË°å | $0.20/1M | $0.20 |
| Bedrock Embeddings | 1000‰∏á„Éà„Éº„ÇØ„É≥ | $0.10/1M | $1.00 |
| Bedrock LLM (Claude Sonnet) | 500‰∏á„Éà„Éº„ÇØ„É≥ | $3.00/1M | $15.00 |
| S3„Çπ„Éà„É¨„Éº„Ç∏ | 100GB | $0.023/GB | $2.30 |
| VPC (NAT Gateway √ó 2) | 2„Ç≤„Éº„Éà„Ç¶„Çß„Ç§ | $32.40/Êúà | $64.80 |
| CloudWatch | 10GB | $0.50/GB | $5.00 |

**Êú¨Áï™Áí∞Â¢ÉÊúàÈ°çÁ∑èË®à: Á¥Ñ $379.18**

## üßπ „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó

### „É™„ÇΩ„Éº„Çπ„ÅÆÂâäÈô§ÊâãÈ†Ü

```bash
#!/bin/bash
# scripts/cleanup-rag-system.sh

PROJECT_NAME="rag-system"
ENVIRONMENT="dev"
REGION="us-east-1"

echo "Cleaning up RAG System resources..."

# S3„Éê„Ç±„ÉÉ„Éà„ÅÆ‰∏≠Ë∫´„ÇíÂâäÈô§
BUCKET_NAME="${PROJECT_NAME}-${ENVIRONMENT}-documents"
aws s3 rm "s3://${BUCKET_NAME}" --recursive --region ${REGION}

# CloudFormation„Çπ„Çø„ÉÉ„ÇØ„ÅÆÂâäÈô§
aws cloudformation delete-stack \
    --stack-name "${PROJECT_NAME}-${ENVIRONMENT}" \
    --region ${REGION}

# „Çπ„Çø„ÉÉ„ÇØÂâäÈô§„ÅÆÂÆå‰∫ÜÂæÖÊ©ü
aws cloudformation wait stack-delete-complete \
    --stack-name "${PROJECT_NAME}-${ENVIRONMENT}" \
    --region ${REGION}

echo "RAG System cleanup completed!"
```

## üìù Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó

### Êé®Â•®„Åï„Çå„ÇãÂ≠¶Áøí„Éë„Çπ
1. **5.2.3 ÁîªÂÉèÁîüÊàêÊ©üËÉΩ**: „Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´AIÊ©üËÉΩ„ÅÆÂÆüË£Ö
2. **6.1.1 „Éû„É´„ÉÅ„Çπ„ÉÜ„Éº„Ç∏„Éì„É´„Éâ**: CI/CD„Éë„Ç§„Éó„É©„Ç§„É≥„Å®„ÅÆÁµ±Âêà
3. **6.2.1 APMÂÆüË£Ö**: È´òÂ∫¶„Å™Áõ£Ë¶ñ„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂàÜÊûê
4. **7.1.1 Claude CodeÂü∫Á§é**: AIÈßÜÂãïÈñãÁô∫„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÅÆÊßãÁØâ

### Áô∫Â±ïÁöÑ„Å™Ê©üËÉΩÂÆüË£Ö
1. **„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´RAG**: „ÉÜ„Ç≠„Çπ„Éà„ÉªÁîªÂÉè„ÉªÈü≥Â£∞„ÅÆÁµ±ÂêàÂá¶ÁêÜ
2. **‰ºöË©±ÂûãRAG**: ÂØæË©±Â±•Ê≠¥„ÇíËÄÉÊÖÆ„Åó„ÅüÁ∂ôÁ∂öÁöÑ„Å™Ë≥™ÂïèÂøúÁ≠î
3. **„Ç®„É≥„Çø„Éº„Éó„É©„Ç§„Ç∫RAG**: Â§ßË¶èÊ®°ÁµÑÁπîÂêë„Åë„ÅÆÊ®©ÈôêÁÆ°ÁêÜ„Å®Áõ£Êüª
4. **„É™„Ç¢„É´„Çø„Ç§„É†RAG**: „Çπ„Éà„É™„Éº„Éü„É≥„Ç∞Ê§úÁ¥¢„Å®ÁîüÊàê„ÅÆÂÆüË£Ö

### ÂÆüË∑µ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ„Ç¢„Ç§„Éá„Ç¢
1. **‰ºÅÊ•≠ÂÜÖ„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ**: Á§æÂÜÖÊñáÊõ∏„ÇíÊ¥ªÁî®„Åó„ÅüË≥™ÂïèÂøúÁ≠î„Ç∑„Çπ„ÉÜ„É†
2. **ÊäÄË°ìÊñáÊõ∏„Ç¢„Ç∑„Çπ„Çø„É≥„Éà**: API‰ªïÊßòÊõ∏„ÇÑ„Éû„Éã„É•„Ç¢„É´„Åã„Çâ„ÅÆËá™ÂãïÂõûÁ≠î
3. **Á†îÁ©∂Ë´ñÊñáÊ§úÁ¥¢„Ç∑„Çπ„ÉÜ„É†**: Â≠¶Ë°ìË´ñÊñá„ÅÆÊÑèÂë≥ÁöÑÊ§úÁ¥¢„Å®Ë¶ÅÁ¥Ñ
4. **Â§öË®ÄË™ûRAG„Ç∑„Çπ„ÉÜ„É†**: Ë§áÊï∞Ë®ÄË™û„Åß„ÅÆÊñáÊõ∏Ê§úÁ¥¢„Å®ÁøªË®≥Ê©üËÉΩ‰ªò„ÅçÂõûÁ≠î

## üîó Â≠¶Áøí„É™„ÇΩ„Éº„Çπ

### AWSÂÖ¨Âºè„Éâ„Ç≠„É•„É°„É≥„Éà
- [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/)
- [Amazon OpenSearch Service Developer Guide](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/)
- [Lambda Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)

### RAG„ÉªAIÈñ¢ÈÄ£„É™„ÇΩ„Éº„Çπ
- [RAG Papers Collection](https://github.com/princeton-nlp/retrieval-augmented-generation)
- [Langchain RAG Tutorials](https://python.langchain.com/docs/use_cases/question_answering/)
- [Vector Database Comparison](https://github.com/erikbern/ann-benchmarks)

### ÂÆüË£ÖÂèÇËÄÉË≥áÊñô
- [Amazon Bedrock Samples](https://github.com/aws-samples/amazon-bedrock-samples)
- [OpenSearch Vector Engine](https://opensearch.org/docs/latest/search-plugins/knn/index/)
- [Serverless RAG Examples](https://github.com/aws-samples/serverless-rag-demo)