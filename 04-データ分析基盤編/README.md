# ãƒ‡ãƒ¼ã‚¿åˆ†æåŸºç›¤ç·¨

## æ¦‚è¦

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€AWSã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ€ãƒ³ãªãƒ‡ãƒ¼ã‚¿åˆ†æåŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã¾ã§ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

## å­¦ç¿’ç›®æ¨™

- ğŸ“Š **ãƒ‡ãƒ¼ã‚¿åé›†**: Kinesisã€EventBridgeã€IoTã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—
- ğŸ”„ **ãƒ‡ãƒ¼ã‚¿å‡¦ç†**: ETL/ELTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€Lambdaã€Glue
- ğŸ—„ï¸ **ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: S3 Data Lakeã€Redshiftã€Athena
- ğŸ“ˆ **ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–**: QuickSightã€CloudWatchã€ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- ğŸ¤– **æ©Ÿæ¢°å­¦ç¿’çµ±åˆ**: SageMakerã€äºˆæ¸¬åˆ†æã€ç•°å¸¸æ¤œçŸ¥

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Sources                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Web/Mobile  â”‚  â”‚     IoT     â”‚  â”‚ External    â”‚  â”‚  APIs   â”‚ â”‚
â”‚  â”‚    Apps     â”‚  â”‚   Devices   â”‚  â”‚    APIs     â”‚  â”‚         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Ingestion                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Kinesis   â”‚  â”‚ EventBridge â”‚  â”‚   Lambda    â”‚  â”‚   SQS   â”‚ â”‚
â”‚  â”‚  Streams    â”‚  â”‚   Events    â”‚  â”‚  Functions  â”‚  â”‚ Queues  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Processing                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    Glue     â”‚  â”‚   Lambda    â”‚  â”‚     EMR     â”‚  â”‚  Step   â”‚ â”‚
â”‚  â”‚  ETL Jobs   â”‚  â”‚ Processing  â”‚  â”‚   Clusters  â”‚  â”‚Function â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Storage                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   S3 Data   â”‚  â”‚  Redshift   â”‚  â”‚  DynamoDB   â”‚  â”‚ RDS     â”‚ â”‚
â”‚  â”‚    Lake     â”‚  â”‚    DWH      â”‚  â”‚   NoSQL     â”‚  â”‚   DB    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Analytics & Visualization                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  QuickSight â”‚  â”‚   Athena    â”‚  â”‚ CloudWatch  â”‚  â”‚ Custom  â”‚ â”‚
â”‚  â”‚ Dashboards  â”‚  â”‚   Queries   â”‚  â”‚  Metrics    â”‚  â”‚   UIs   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å­¦ç¿’ãƒ‘ã‚¹

### 4.1 ãƒ‡ãƒ¼ã‚¿åé›†ã¨ä¿å­˜
- **4.1.1** Kinesisã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- **4.1.2** ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆGlueï¼‰

### 4.2 å¯è¦–åŒ–ã¨åˆ†æ
- **4.2.1** QuickSightãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- **4.2.2** CloudWatchãƒ¡ãƒˆãƒªã‚¯ã‚¹

### 4.3 é«˜åº¦ãªåˆ†æï¼ˆæ‹¡å¼µï¼‰
- **4.3.1** Athenaã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
- **4.3.2** SageMakeræ©Ÿæ¢°å­¦ç¿’çµ±åˆ

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### å‰ææ¡ä»¶

```bash
# AWS CLIç¢ºèª
aws --version

# Pythonç’°å¢ƒç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ï¼‰
python3 --version
pip3 --version

# Node.jsç¢ºèªï¼ˆLambdaé–¢æ•°ç”¨ï¼‰
node --version
npm --version
```

### å…¨ä½“ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
# ãƒ‡ãƒ¼ã‚¿åˆ†æåŸºç›¤ã®ä¸€æ‹¬ãƒ‡ãƒ—ãƒ­ã‚¤
./scripts/deploy-all-infrastructure.sh deploy-data

# æ®µéšçš„ãƒ‡ãƒ—ãƒ­ã‚¤
./scripts/deploy-data-platform.sh deploy-ingestion
./scripts/deploy-data-platform.sh deploy-processing
./scripts/deploy-data-platform.sh deploy-analytics
```

## å­¦ç¿’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è©³ç´°

### ğŸ“Š 4.1.1 Kinesisã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

**å­¦ç¿’å†…å®¹:**
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- Kinesis Data Streamsè¨­è¨ˆ
- Kinesis Analytics ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ
- ä¸¦åˆ—å‡¦ç†ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

**å®Ÿè£…å†…å®¹:**
- Kinesis Data Streamsæ§‹ç¯‰
- Lambda ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼å®Ÿè£…
- Kinesis Analytics ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- CloudWatchç›£è¦–è¨­å®š

**ä¸»è¦æŠ€è¡“:**
- Amazon Kinesis Data Streams
- Amazon Kinesis Analytics
- AWS Lambda
- Amazon CloudWatch

### ğŸ”„ 4.1.2 ETLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**å­¦ç¿’å†…å®¹:**
- ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†ã®è¨­è¨ˆ
- AWS Glue ã«ã‚ˆã‚‹ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ ETL
- ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ç®¡ç†
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ä¾å­˜é–¢ä¿‚ç®¡ç†

**å®Ÿè£…å†…å®¹:**
- Glue ETL ã‚¸ãƒ§ãƒ–å®Ÿè£…
- Glue Data Catalog è¨­å®š
- Step Functions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- å“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¸»è¦æŠ€è¡“:**
- AWS Glue
- AWS Step Functions
- Amazon S3
- AWS Lambda

### ğŸ“ˆ 4.2.1 QuickSightãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

**å­¦ç¿’å†…å®¹:**
- ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹è¨­è¨ˆ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­è¨ˆã¨æœ€é©åŒ–
- çµ„ã¿è¾¼ã¿åˆ†æã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

**å®Ÿè£…å†…å®¹:**
- QuickSight ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆã¨å®Ÿè£…
- è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- åŸ‹ã‚è¾¼ã¿åˆ†ææ©Ÿèƒ½

**ä¸»è¦æŠ€è¡“:**
- Amazon QuickSight
- Amazon Athena
- Amazon Redshift
- S3 Data Lake

### ğŸ“Š 4.2.2 CloudWatchãƒ¡ãƒˆãƒªã‚¯ã‚¹

**å­¦ç¿’å†…å®¹:**
- ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­è¨ˆ
- ã‚¢ãƒ©ãƒ¼ãƒˆã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- ãƒ­ã‚°åˆ†æã¨ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
- ã‚³ã‚¹ãƒˆç›£è¦–ã¨æœ€é©åŒ–

**å®Ÿè£…å†…å®¹:**
- ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…
- CloudWatch ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
- ã‚¢ãƒ©ãƒ¼ãƒ ã¨SNSé€šçŸ¥
- ãƒ­ã‚°é›†ç´„ã¨åˆ†æ

**ä¸»è¦æŠ€è¡“:**
- Amazon CloudWatch
- CloudWatch Logs
- Amazon SNS
- AWS X-Ray

## ğŸ—ï¸ å®Ÿè£…æ‰‹é †

### Step 1: ãƒ‡ãƒ¼ã‚¿åé›†å±¤ã®æ§‹ç¯‰

```bash
# Kinesisã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆ
aws kinesis create-stream \
  --stream-name data-ingestion-stream \
  --shard-count 2

# ãƒ‡ãƒ¼ã‚¿ç”ŸæˆLambdaé–¢æ•°ãƒ‡ãƒ—ãƒ­ã‚¤
cd 04-ãƒ‡ãƒ¼ã‚¿åˆ†æåŸºç›¤ç·¨/4.1-ãƒ‡ãƒ¼ã‚¿åé›†ã¨ä¿å­˜/4.1.1-Kinesisã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
aws cloudformation create-stack \
  --stack-name data-ingestion \
  --template-body file://cloudformation/kinesis-streaming.yaml \
  --capabilities CAPABILITY_IAM
```

### Step 2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†å±¤ã®æ§‹ç¯‰

```bash
# Glue ETLã‚¸ãƒ§ãƒ–ã®è¨­å®š
aws glue create-job \
  --name data-transformation-job \
  --role GlueServiceRole \
  --command ScriptLocation=s3://my-bucket/etl-script.py

# Step Functions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ
aws stepfunctions create-state-machine \
  --name data-processing-workflow \
  --definition file://step-functions-definition.json
```

### Step 3: ãƒ‡ãƒ¼ã‚¿ä¿å­˜å±¤ã®æ§‹ç¯‰

```bash
# S3 Data Lake ãƒã‚±ãƒƒãƒˆä½œæˆ
aws s3 mb s3://my-data-lake-bucket
aws s3api put-bucket-encryption \
  --bucket my-data-lake-bucket \
  --server-side-encryption-configuration file://encryption-config.json

# Athena ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
aws athena start-query-execution \
  --query-string "CREATE DATABASE analytics_db"
```

### Step 4: åˆ†æãƒ»å¯è¦–åŒ–å±¤ã®æ§‹ç¯‰

```bash
# QuickSight ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
aws quicksight create-data-set \
  --aws-account-id 123456789012 \
  --data-set-id my-dataset \
  --name "Analytics Dataset"

# CloudWatch ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
aws cloudwatch put-dashboard \
  --dashboard-name "DataPlatformDashboard" \
  --dashboard-body file://dashboard-config.json
```

## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒªãƒ³ã‚°

### ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯è¨­è¨ˆ

```
s3://my-data-lake/
â”œâ”€â”€ raw/                    # ç”Ÿãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ year=2024/
â”‚   â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”‚   â”œâ”€â”€ day=15/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ events/
â”‚   â””â”€â”€ source=app/
â”œâ”€â”€ processed/              # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ aggregated/
â”‚   â”œâ”€â”€ cleaned/
â”‚   â””â”€â”€ enriched/
â””â”€â”€ curated/               # ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿
    â”œâ”€â”€ analytics/
    â”œâ”€â”€ ml/
    â””â”€â”€ reporting/
```

### ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥

```sql
-- Athena ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¾‹
CREATE TABLE events (
  event_id string,
  user_id string,
  event_type string,
  timestamp timestamp,
  properties map<string,string>
)
PARTITIONED BY (
  year int,
  month int,
  day int,
  source string
)
STORED AS PARQUET
LOCATION 's3://my-data-lake/processed/events/'
```

## ğŸ§ª ãƒ‡ãƒ¼ã‚¿å“è³ªç®¡ç†

### ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

```python
# Glue ETL ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
def validate_data(df):
    # NULLå€¤ãƒã‚§ãƒƒã‚¯
    null_count = df.filter(df.user_id.isNull()).count()
    if null_count > 0:
        raise ValueError(f"NULL values found: {null_count}")
    
    # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
    numeric_columns = ['amount', 'quantity']
    for col in numeric_columns:
        if df.filter(~df[col].cast('double').isNotNull()).count() > 0:
            raise ValueError(f"Invalid numeric values in {col}")
    
    return True

# ETLå‡¦ç†
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="analytics_db",
    table_name="raw_events"
)

# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
df = datasource.toDF()
validate_data(df)  # å“è³ªãƒã‚§ãƒƒã‚¯

# å¤‰æ›å‡¦ç†
transformed_df = df.filter(df.event_type != 'test') \
                  .withColumn('processed_at', current_timestamp())

# å‡ºåŠ›
output_frame = DynamicFrame.fromDF(transformed_df, glueContext, "output")
glueContext.write_dynamic_frame.from_options(
    frame=output_frame,
    connection_type="s3",
    connection_options={
        "path": "s3://my-data-lake/processed/events/",
        "partitionKeys": ["year", "month", "day"]
    },
    format="parquet"
)

job.commit()
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### Kinesis æœ€é©åŒ–

```yaml
# Kinesisè¨­å®šä¾‹
KinesisStream:
  ShardCount: !Ref ShardCount  # æ›¸ãè¾¼ã¿é€Ÿåº¦ã«å¿œã˜ã¦èª¿æ•´
  RetentionPeriod: 168         # 7æ—¥é–“ä¿æŒ
  ShardLevelMetrics:
    - IncomingRecords
    - OutgoingRecords
```

### Athena æœ€é©åŒ–

```sql
-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å°„å½±ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿã‚¯ã‚¨ãƒª
CREATE TABLE optimized_events (
  event_id string,
  user_id string,
  event_type string,
  properties string
)
PARTITIONED BY (
  year int,
  month int,
  day int
)
STORED AS PARQUET
LOCATION 's3://my-data-lake/optimized/events/'
TBLPROPERTIES (
  'projection.enabled'='true',
  'projection.year.type'='integer',
  'projection.year.range'='2020,2030',
  'projection.month.type'='integer',
  'projection.month.range'='1,12',
  'projection.day.type'='integer',
  'projection.day.range'='1,31',
  'storage.location.template'='s3://my-data-lake/optimized/events/year=${year}/month=${month}/day=${day}/'
);
```

## ğŸ” ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç›£è¦–

```bash
# CloudWatch ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
aws cloudwatch put-metric-data \
  --namespace "DataPipeline" \
  --metric-data MetricName=ProcessedRecords,Value=1000,Unit=Count

# ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ
aws cloudwatch put-metric-alarm \
  --alarm-name "DataQualityFailure" \
  --alarm-description "Data quality check failed" \
  --metric-name "QualityCheckFailures" \
  --namespace "DataPipeline" \
  --statistic "Sum" \
  --period 300 \
  --threshold 1 \
  --comparison-operator "GreaterThanOrEqualToThreshold"
```

### ã‚³ã‚¹ãƒˆç›£è¦–

```python
# Lambdaé–¢æ•°ã§ã‚³ã‚¹ãƒˆç›£è¦–
import boto3
import json

def lambda_handler(event, context):
    ce = boto3.client('ce')
    
    # S3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆå–å¾—
    response = ce.get_cost_and_usage(
        TimePeriod={
            'Start': '2024-01-01',
            'End': '2024-01-31'
        },
        Granularity='MONTHLY',
        Metrics=['BlendedCost'],
        GroupBy=[
            {
                'Type': 'DIMENSION',
                'Key': 'SERVICE'
            }
        ]
    )
    
    # ã‚³ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆ
    for group in response['ResultsByTime'][0]['Groups']:
        service = group['Keys'][0]
        cost = float(group['Metrics']['BlendedCost']['Amount'])
        
        if service == 'Amazon Simple Storage Service' and cost > 1000:
            sns = boto3.client('sns')
            sns.publish(
                TopicArn='arn:aws:sns:us-east-1:123456789012:cost-alerts',
                Message=f'S3 cost exceeded $1000: ${cost:.2f}',
                Subject='High S3 Cost Alert'
            )
    
    return {'statusCode': 200}
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. Kinesis ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

```python
# æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ãƒªãƒˆãƒ©ã‚¤
import time
import random

def put_record_with_retry(kinesis_client, stream_name, data, partition_key, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = kinesis_client.put_record(
                StreamName=stream_name,
                Data=data,
                PartitionKey=partition_key
            )
            return response
        except kinesis_client.exceptions.ProvisionedThroughputExceededException:
            if attempt < max_retries - 1:
                sleep_time = (2 ** attempt) + random.uniform(0, 1)
                time.sleep(sleep_time)
            else:
                raise
```

#### 2. Glue ETL ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼

```python
# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€é©åŒ–
def optimize_dataframe(df):
    # ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col], downcast='integer')
            except:
                pass
    
    # ä¸è¦ã‚«ãƒ©ãƒ ã®å‰Šé™¤
    df = df.drop(['temp_column'], axis=1, errors='ignore')
    
    return df
```

#### 3. Athena ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

```sql
-- ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
-- 1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å°„æ˜ ã®ä½¿ç”¨
-- 2. ã‚«ãƒ©ãƒ ãƒŠãƒ¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆParquetï¼‰ã®ä½¿ç”¨
-- 3. é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å‹ã®é¸æŠ

-- åŠ¹ç‡çš„ãªã‚¯ã‚¨ãƒªä¾‹
SELECT 
    event_type,
    COUNT(*) as event_count
FROM events
WHERE year = 2024 
    AND month = 1 
    AND day BETWEEN 1 AND 7
GROUP BY event_type
LIMIT 1000;
```

## ğŸ’° ã‚³ã‚¹ãƒˆæœ€é©åŒ–

### S3 ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–

```yaml
# ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ãƒãƒªã‚·ãƒ¼
LifecycleConfiguration:
  Rules:
    - Id: DataArchiving
      Status: Enabled
      Transitions:
        - Days: 30
          StorageClass: STANDARD_IA
        - Days: 90
          StorageClass: GLACIER
        - Days: 365
          StorageClass: DEEP_ARCHIVE
      ExpirationInDays: 2555  # 7å¹´å¾Œå‰Šé™¤
```

### Kinesis ã‚³ã‚¹ãƒˆæœ€é©åŒ–

```bash
# ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ãƒ¢ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰
aws kinesis put-record \
  --stream-name my-stream \
  --data "sample data" \
  --partition-key "key1"

# ã‚·ãƒ£ãƒ¼ãƒ‰æ•°å‹•çš„èª¿æ•´
aws application-autoscaling register-scalable-target \
  --service-namespace kinesis \
  --resource-id stream/my-stream \
  --scalable-dimension kinesis:stream:shard-count
```

## ğŸ“š å‚è€ƒè³‡æ–™

### AWS ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [Big Data Analytics Options on AWS](https://aws.amazon.com/big-data/analytics-options/)
- [Data Lake Implementation Guide](https://aws.amazon.com/solutions/implementations/data-lake-solution/)
- [Analytics Lens - Well-Architected Framework](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/)

### ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- [The Data Engineering Handbook](https://github.com/DataExpert-io/data-engineer-handbook)
- [Apache Parquet Best Practices](https://parquet.apache.org/docs/)
- [Data Modeling for Analytics](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/)

## ğŸ“ˆ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

å®Œäº†å¾Œã¯ä»¥ä¸‹ã«é€²ã‚“ã§ãã ã•ã„ï¼š

1. **[AI-MLçµ±åˆç·¨](../05-AI-MLçµ±åˆç·¨/README.md)** - æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
2. **[CI-CDé«˜åº¦åŒ–ç·¨](../06-CI-CDé«˜åº¦åŒ–ç·¨/README.md)** - è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
3. **[Claude Code & Bedrockç·¨](../07-Claude-Code-Bedrock-AIé§†å‹•é–‹ç™ºç·¨/README.md)** - AIé§†å‹•é–‹ç™º

---

## ğŸ¯ å­¦ç¿’ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ãƒ‡ãƒ¼ã‚¿åé›†
- [ ] Kinesis Data Streamsè¨­å®š
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- [ ] ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- [ ] ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- [ ] Glue ETL ã‚¸ãƒ§ãƒ–è¨­è¨ˆ
- [ ] ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè£…
- [ ] ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†

### ãƒ‡ãƒ¼ã‚¿ä¿å­˜
- [ ] Data Lake è¨­è¨ˆ
- [ ] ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥
- [ ] ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ç®¡ç†
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

### åˆ†æãƒ»å¯è¦–åŒ–
- [ ] QuickSight ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
- [ ] Athena ã‚¯ã‚¨ãƒªæœ€é©åŒ–
- [ ] ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®Ÿè£…
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

**æº–å‚™ãŒã§ããŸã‚‰æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¸é€²ã¿ã¾ã—ã‚‡ã†ï¼**